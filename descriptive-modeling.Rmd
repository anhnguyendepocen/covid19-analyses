# Descriptive Modeling {#descmodel}

The previous chapter presented several descriptive plots of raw data.  What I'd like to do in this chapter is show some simple modeling methods. 
By no means is any of the modeling presented here considered breakthrough modeling. I call this descriptive modeling because all I do is describe the patterns using statistical methods. In the next chapter I'll give some examples of models that attempt to capture mechanism. Another way of explaining the difference between descriptive models and explanatory models is that the former are summaries of the data and can be used to forecast, but they do not provide  understanding of the underling mechanisms so they do not give insight into what would happen if a change was imposed on the system.  Descriptive models don't tell us the effect social distancing may have, for example, on the future rate of covid-19. They are fine for describing what has happened in the past but can be limited in helping us understand how the future will play out under various scenarios including interventions. I sometimes refer to descriptive modeling as "models of data."  Process models  are about modeling process, or underlying mechanism; they use data to estimate parameters and test predictions.

I'll try to use only the R features necessary to make the points I want to highlight. There are more complicated approaches using time series packages that format the data differently (e.g., they use special time series objects in R). There are benefits of doing this because there are automatic plotting and estimation routines that can take time series objects and do relevant computations. But I think as you are learning this it makes more sense to learn how to do some of the programming and then you are in a better position to learn the value of more advanced approaches like those for time series data.

I won't cover commonly used descriptive summary methods such as smoothing techniques, mostly in the interest of time.  For more general treatments on working with time series data see [Hyndman & Athanasopoulos](https://otexts.com/fpp2/); [Holmes, Scheuerell & Ward](https://nwfsc-timeseries.github.io/atsa-labs/) and the classic [Box & Jenkins](https://www.amazon.com/Time-Analysis-Forecasting-George-Box/dp/0470272848), with additional authors in newer editions.

##  Regression on US State Percentages

In the chapter [Descriptive Plots](#descplots) I showed that on the log scale the curves look  like they can be approximated by straight lines by state for the count/population variable.

These data are a natural candidate for a random effects linear regression.  Each state should have its own intercept and slope.  We could create a complicated interaction between day and state, so that we have a state factor with 50 levels (so 49 effect or contrast codes), giving us 49 intercepts and slopes but that is overkill. The random effects approach makes more sense.  Each state is modeled as a random draw from a multivariate normal distribution such that we draw 50 pairs of slopes and intercepts from that multivariate distribution of intercept and slope. Once we have those 50 slope and intercept pairs, then we compute the predicted scores for each state. The algorithm finds the mean and covariance matrix of that multivariate distribution to minimize the discrepancy (i.e., squared residuals) between the observed  and the predicted data.  As shown in the previous chapter, if logs are taken of the percentages, then the state-level data appear to follow straight lines.

Day is entered into the model as a linear time variable (1, 2, 3, etc). Our previous plots suggest that on the log scale we may be fine with linear time as a predictor.


```{r echo=longform}
#drop all 0 counts from the allstates.long data base. be careful, this data.frame no longer has 0s
#can't study liftoff (see on descriptive plots) now that 0s are removed
#reason is that log10(0) = -Inf
allstates.long <- allstates.long %>% mutate(count = na_if(count, "0"))

#save percent variable and log10 version
allstates.long$percent <- allstates.long$count/allstates.long$population * 100
allstates.long$percent.log10 <- log10(allstates.long$percent)
```

It is good to check the distribution of the log percent variable to make sure there aren't major issues with symmetry. 

```{r echo=longform}
#histogram of percent.log10; not a bad looking distribution
hist(allstates.long$percent.log10, xlab="log percent", main="Histogram")
```

Run the linear mixed model on the log scale and examine the summary of the model.

```{r echo=longform}
#run linear mixed model default optimizer didn't converge so switched to Nelder_Mead
out.lmer <- lmer(percent.log10 ~ 1 + day.numeric + (1+ day.numeric|state.abb), allstates.long, control = lmerControl(optimizer ="Nelder_Mead"))

#summary(out.lmer)
#tab_model() produces nicer looking tables
if (is_html_output())
tab_model(out.lmer, show.se=T, show.stat=T)
```

<!-- tab_model doesn't work for latex/pdf so switch to stargazer for formatted table -->

```{r results="asis", echo=is_latex_output(), eval=is_latex_output() }
if (is_latex_output())
stargazer(out.lmer, header=F, type="latex")
```

The estimates, se(est), CI, t and pvalue for the fixed effects are printed first, then the parameters of the random effect part. In order, they are the variance of the residuals, the variance of the random effect intercept, the variance of the random effect slope, the correlation between the random intercept and the random slope, the adjusted ICC, number of states, number of observations (recall 0s have been removed from this data file so the N refers to the number of days by state for which there was a nonzero number of positive covid-19 cases), and the R$^2$ values.  
```{asis echo=longform}
For more documentation on the computation of these terms see the help page for function tab_model().
```

Now let's examine plots of residuals to diagnose potential issues.

```{r echo=longform}
#plots of residuals
plot(out.lmer)
qqnorm(resid(out.lmer))
qqline(resid(out.lmer))
```

The residuals against the fitted values suggest a nonlinear pattern remains in the data. This suggests that we may not have completely linearized by the log transformation. The qqplot suggests there may be a long tail.  Both of these plots suggest there are issues that have to be addressed.  We probably don't want to jump right into doing another transformation right way as that may create new issues with interpretation (we selected the log transformation because we are checking for exponential growth and on the log scale the exponential growth should become linear). So maybe the exponential growth model may not be appropriate here.


Let's look at the state-level slopes and intercepts more directly:  first with a table of each state and then with some plots.  Light gray points have confidence intervals that overlap 0 so those parameter estimates are not significantly different from zero. But here we did not do a correction like Bonferroni or Scheffe to address the 100 tests we just did (50 states on each of slope and intercept).  This would just require a small modification to the code to adjust the level of the CI to correspond, say, to the Bonferroni corrected levels.

```{r echo=longform}
#list of the set of 50 intercept/slope pairs
coef(out.lmer)$state.abb

#quantile plot of the estimates
#ggCaterpillar(ranef(out.lmer, condVar=T),QQ=T, likeDotplot = T)

#include 95% CI, could do instead +/- 1 standard error, which is roughly .68 CIs
#light grey points are not statistically different from 0
raeff <- REsim(out.lmer)
plotREsim(raeff,labs=T, stat="mean",level=.95)
```


It turns out that computing standard errors for the prediction is not straightforward for linear mixed models and quite complicated for nonlinear mixed models that we will perform later.  You can read about the complexity [here](https://stats.stackexchange.com/questions/235018/r-extract-and-plot-confidence-intervals-from-a-lmer-object-using-ggplot).

Here is one method for computing the predicted value and a prediction interval. To maintain clarity I'll focus just on two states, but one could do these plots for all 50 states.

```{r warning=F, message=F, echo=longform}
#First New York
nydf <- allstates.long %>% filter(state.abb == "New York")
pred <- data.frame(day.numeric= nydf$day.numeric, predictInterval(out.lmer, nydf, which="full", level=.95), percap100=nydf$percap100)
pred

ggplot(pred) + 
  geom_line(aes(day.numeric, fit)) +
  geom_ribbon(aes(day.numeric, ymin = lwr, ymax = upr), alpha = .2) +
  geom_point(aes(day.numeric, y = log10(percap100)))+ ggtitle("New York")
```

```{r  warning=F, message=F,  echo=longform, eval=longform}
#similar plot adding prediction band 5 days out beyond current day
#but this doesn't look right as the band should widen the 5 days beyond data
outdays <- 5
pred2 <- data.frame(day.numeric = 1:(max(nydf$day.numeric)+outdays), predictInterval(out.lmer, newdata=list(day.numeric=1:(max(nydf$day.numeric)+outdays), state.abb=rep("New York",outdays+max(nydf$day.numeric))), which="full", level=.95))
pred2

ggplot(pred2) + 
  geom_line(aes(day.numeric, fit)) +
  geom_ribbon(aes(day.numeric, ymin = lwr, ymax = upr), alpha = .2) +
  geom_point(data=nydf, aes(day.numeric, y = log10(percap100)))+ ggtitle("New York")

```

Note that the linear pattern (line and prediction band) doesn't do a good job of capturing the actual data points. Some points are under predicted in the early days, then overpredicted, then underpredicted again. This would produce a pattern where the residuals are negative, then positive, then negative again as we saw in one of the residual plots.  The data looked approximately linear but upon close inspection a model that is actually linear in the log scale seems to systematically miss the pattern.  

To check this is not just a fluke for New York, let's look at another state. For Michigan a similar story, where we see under, over and under prediction. These plots suggest while the linear model may seem ok at first pass, there may some systematic deviations form the line that are worth capturing.  We can see that if we forecast out five days or so, the line will continue to increase but it appears that the points are increasing at a slower rate.

```{r  warning=F, message=F, echo=longform}
#similarly for michigan
midf <- allstates.long %>% filter(state.abb == "Michigan")
pred <- data.frame(day.numeric=midf$day.numeric, predictInterval(out.lmer, midf), percap100=midf$percap100)
pred

ggplot(pred) + 
  geom_line(aes(day.numeric, fit)) +
  geom_ribbon(aes(day.numeric, ymin = lwr, ymax = upr), alpha = .2) +
  geom_point(aes(day.numeric, y = log10(percap100))) + ggtitle("Michigan")
```

So these plots suggest a major issue with this regression. The observation that the trajectories do not display a straight line in the log scale suggests these data do not follow an exponential growth model. It is more complicated than that.  We see similar violations of straight lines in the death data (e.g., see the New York Times [graphs on the number of deaths](https://www.nytimes.com/interactive/2020/03/21/upshot/coronavirus-deaths-by-country.html?action=click&module=moreIn&pgtype=Article&region=Footer&fbclid=IwAR3y14SlV9sBtQcAyugUc0LPU-mrIVJbTrzOi4t7rWuISt8A-xizCMEk_7Y) for a similar curvature away from a straight line).  The violation of the exponential could be due to many factors, such as interventions of social distancing, diseases tend to follow an exponential curve early but then switch to a different process (see below for logistic growth curves), etc.

Now that we understand the slopes and intercepts, let's make some predictions.  I'll go 5 days out.  The first plot will be on the log scale, but be mindful that these predictions are based on an exponential model that we just saw evidence that it may not hold for these data.

```{r fig.cap="Log scale predictions 5 days out", echo=longform}
max.day.numeric <- max(allstates.long$day.numeric)
max.day <- max(allstates.long$day)

#predict day.ahead number of days
days.ahead <- 5

#be sure order of states is the same across all parts of this command
predmat <- data.frame(day.numeric=rep(c(0:(max.day.numeric+days.ahead)),50),state.abb=rep(rownames(coef(out.lmer)$state.abb),each=max.day.numeric+days.ahead+1), prettyval=rep(allstates.long$prettyval[order(match(allstates$`Province/State`,state.population$NAME))], each=max.day.numeric+days.ahead+1),
population= rep(allstates.long$population[order(match(allstates$`Province/State`,state.population$NAME))], each=max.day.numeric+days.ahead+1))

#compute predictions
predmat$prediction <- predict(out.lmer, predmat)

#linear plot of predictions as estimated
predmat %>% mutate(label = if_else(day.numeric == max(day.numeric), as.character(prettyval), NA_character_)) %>%
  ggplot(aes(day.numeric,prediction,group=prettyval, color=prettyval )) +  geom_line()   +
   geom_label_repel(aes(label= label), nudge_x=2, na.rm=T,segment.color = 'grey50', label.size=.01, size=2.5, show.legend=F) +
coord_cartesian(clip = 'off') +  scale_x_continuous(limits=c(0,max.day.numeric+ days.ahead+3)) +
    theme(legend.position="none", plot.margin = margin(0.1, 1, 0.1, 0.1, "cm")) + geom_vline(xintercept=max.day.numeric+.5,color="red") +
  annotate(geom="text",x=max.day.numeric-6,y=-5, label="Days with observations", color="red") +
  annotate(geom="text",x=max.day.numeric+4,y=-5, label="Predictions", color="red") 
```

Using the same data I  take the exponential of those predictions to see the percentages with their predicted curves.  The Y axis is on the percentage scale (count/population * 100).

```{r fig.cap="Predictions of percentages 5 days out", echo=longform}
#plot of same predictions as 10^prediction to put back on original percentage scale
predmat %>% mutate(label = if_else(day.numeric == max(day.numeric), as.character(prettyval), NA_character_)) %>%
ggplot( aes(day.numeric,10^prediction,group=prettyval, color=prettyval )) +  geom_line()   +
   geom_label_repel(aes(label= label), nudge_x=2, na.rm=T,segment.color = 'grey50', label.size=.01, size=2.5, show.legend=F) +
coord_cartesian(clip = 'off') +  scale_x_continuous(limits=c(0,max.day.numeric+ days.ahead+3)) +
    theme(legend.position="none", plot.margin = margin(0.1, 1, 0.1, 0.1, "cm")) + geom_vline(xintercept=max.day.numeric+.5,color="red") +
  annotate(geom="text",x=max.day.numeric-6,y=4, label="Days with observations", color="red") +
  annotate(geom="text",x=max.day.numeric+4,y=4, label="Predictions", color="red") 
```

Other approaches I could have taken include using the state's population as a weight and computed weighted linear mixed models on the log of the count data, or, because I'm working in log scale, I could use the state population as an offset in the linear mixed model (as in offset(log(population))) and use log counts as the dependent variable.  These are alternative ways of taking population size into account.  Further checks include taking into the account possible correlated residuals. For example, it isn't reasonable to assume that the residual at day t is independent from the residual at day t+1 for a given state, which is what my model assumed in what I presented above.   There is clearly a lot more one would need to do in order to justify the use of the analytic strategy in this section.

Of course, once the basic structure of this linear mixed model on the log scale is set (e.g., error structure on residuals is determined, how best to use the state's population in the model, etc), then one could bring in additional predictors. We could, for example, take some of the data sets we downloaded that has the dates of when states took different measures and see if any of those changed, say, the slopes for the states.  At this point, once the complex error structure, linearity, and random effect structure are determined, then the rest is just routine regression that follows the analogous criteria in regression.
```{asis echo=longform}
For example, if you have interactions, center your variables; be careful using dummy codes; if you add covariates as control variables, check the parallelism assumption; double check the residuals to make sure they follow a normal distribution, don't show weird nonlinear patterns, don't exhibit outliers, etc.
```

### Comparison with polynomial regression

In the previous model I showed that the linear mixed model (allowing for each state to have their own slope and intercept) on the log of the state-level percentages did a reasonable job of fitting the data.  Here I want to do a little tangent and illustrate how well a polynomial can mimic the exponential pattern we see in the data.  

Let's take the case that in a particular locale the rate of covid-19 follows this general curve. In this subsection I made up the data so I know the curve follows exponential growth with an additive error term (the plus $\epsilon$) because I added random normally distributed noise with mean 0 and standard deviation .25.

Overview of what follows in the remainder of this subsection: I'll run several regressions in order and produce one plot at the end.  The points on the graph are the observed data points, the colored curves emerge from different regression models.    The first regression is  a linear regression with just x (days 1-25) as a linear predictor. I added a violet straight line to indicate the best fitting straight line--not a good fit.  The second regression adds a quadratic term and will appear as a red curve. This is an improvement, but it is nonmonotonic and shows a decrease in the first few days (which is not a pattern displayed in the raw data).  The third regression adds a cubic term and fits the points very well.

Next, I estimate the exponential directly using nonlinear regression. Instead of the parameter being a multiplier of the predictor (like in linear regression), the parameter serves as the growth factor. The green curve follows the data closely. Here only one parameter was estimated and it was .1502, very close to the actual .15 I used to estimate the data.  The polynomial did quite well but needed 4 parameters (intercept and three slopes); those parameters are not easy to interpret. The single parameter of the exponential yields a very good fit with only one _interpretable_ parameter.  Note that here I am using the nls() command, which estimates a nonlinear regression.

Finally, for comparison, I run a linear regression through these data after transforming the Y variable into the log, then re-expressing the predicted scores back to the original scale like I did in the predictions using the linear mixed model with the lmer() command above.  I'll draw that curve in black---very similar result to the nonlinear regression with the exponential and the third order polynomial.


```{r origplot, echo=longform, results=ifelse(longform,"markup","hide")}
#set random seed to results appear the same each run
set.seed(0101202019)
x <- 1:25
#here is the true model that generated the data
y <- 1*exp(.15*x) + rnorm(length(x),0,.25)
plot(x,y)

#linear regression with one predictor, draw violet curve
out <- lm(y~x)
summary(out)
lines(x,predict(out), col="violet")

#add quadratic term, draw red curve
out <- lm(y~x + I(x^2))
summary(out)
lines(x,predict(out), col="red")

#add cubic term, draw blue curve
out <- lm(y~x + I(x^2) + I(x^3))
summary(out)
lines(x,predict(out), col="blue")


#nonlinear exponential directly estimated, draw green curve
out <- nls(y~exp(beta*x), start=list(beta=.5))
summary(out)
lines(x,predict(out), col="green")

#log transform followed by linear regression, draw black curve
out <- lm(log(y)~x)
summary(out)
lines(x,exp(predict(out)), col="black")
```

Polynomials can do a good job at capturing functions but they are not always easy to interpret. The third order polynomial got the pattern relatively closely. The quadratic though while it was close predicts that the counts in the early days will drop (the red curves is decreasing from day 1 to day 4) so that's not good because the data points don't see to be showing that pattern.  
```{asis echo=longform}
The nonlinear model (nls) estimated the parameters quite closely (the true multiplier was .15 and that is what we estimated; the standard error of the residuals was .25 and it was estimated as .29). The other highlight is that the linear model on the log data (black curve) was almost indistinghisable from the true exponential pattern as estimated by the fits of the green line.  The estimates from the linear model (lm) on the log transformed data are also quite close to the true value. The intercept in the log linear model should correspond to the multiplier in the true model, which is 1, and the exponential of the intercept is .98 (exp(-.0183)). The value of the multiplier in the log linear model (.1512) is close to the true multiplier in the model I created .15.
```

I found this website that uses a [4th order polynomial to model data from Spain, Italy and US](https://www.mmogollon.com/corona-virus-2). This page also focuses on the daily incidence rate, a topic I mentioned earlier. Personally, I'm suspicious of using such high order terms to model such curves especially if they are used to forecast out from the data (i.e., extrapolation).

The polynomial idea to approximate functions can be extended in more complex ways. [Here](http://uc-r.github.io/mars) is a tutorial using the MARS package in R; this is probably the direction I'd go if I was interested in taking this type of approximation approach.

### Thoughts on the log transformation

This type of model where we transform the dependent variable by converting to logs, creates an additive error term in the log scale (i.e., log(counts) = f(days) + $\epsilon$). But maybe that isn't the right error structure. We can study the residuals to see if there are nonlinearities. It may be that the error is added to the counts rather than the log of the counts (i.e., the data may exhibit a  counts = f(days) + $\epsilon$ structure, which is a different error structure than the one on the log scale).  

We now turn to modeling the curvature directly, in a form that permits the counts = f(days) + $\epsilon$ model.

## Exponential (Nonlinear) Regression 

```{asis echo=longform}
In the previous section I illustrated how to run a nonlinear regression using the nls() command to estimate exponential growth. 
```

### Exponential Background {#expoback}

For a good explanation of exponential growth and logistic growth (next subsection) see this [video](https://www.youtube.com/watch?v=Kas0tIxDvrg). 

You can think of this model as being similar to a balance in a savings account that builds with compound interest. At each time step, a percentage is added to the current principal thus increasing the balance in the savings account. At the subsequent time step, the principal is now higher so the interest earned on that day will also be higher.  In the case of the covid-19 counts, each day the total grows by some percentage, analogous to the principal in the savings account.

```{r messages=F, warnings=F, echo=longform}
allstates.long.new <- groupedData(count ~ day.numeric | state.abb, data=allstates.long)

#this is a more general function that can estimate liftoff l (point at which counts begin to be nonzero)
#for for now set l=0 
richfu <- function(d,l,rate,y,b1){
ifelse(d<l,0,y*rate^(b1*(d-l)))
}

#example exponential growth model
plot(0:max.day.numeric, richfu(0:max.day.numeric, 0, 1.23 , .000555 , 1)*19500000/100,type="l",xlab="days",ylab="counts",main="Hypothetical plot of counts following an exponential growth \n(tied to population of New York)")
```

I'm using one form of the exponential where the "rate" parameter is the base and it is raised to the power t.  I did this to maintain a connection to the savings account analogy where the principal grows at rate x (where, say, a 5% interest rate has x = 1.05). The form is

$$
\begin{aligned}
\mbox{count} &= y r^t
\end{aligned}
$$
where t is the day, r is the rate (i.e., an "interest rate" of 5% has rate = 1.05), and y is a scaling parameter. The hypothetical plot above used a rate of 1.23 and a multiplier of 108.225, which is in part based on the population of NY of 19.5M.

There are other forms of the expontial that are equivalent, just on a different scale.  For example, in the natural log scale, the form I'm using $yr^t$ is equivalently expressed as $y*\exp{(log(r)t)}$ and in base 10 it is equivalently expressed as $y*10^{log10(r)t}$. These are all equivalent formulations, one just needs to keep track of the scale of the rate across these different choices of the base.


### Exponential: The Liftoff Parameter

The exponential function can vary in terms of liftoff, where the curve breaks away from 0. 
```{asis echo=longform}
Liftoff is the parameter "l" I included in the richfu function in the previous code chunk.   
```
Here I plot two hypothetical states that have the identical set of parameters except that one state (black) has a liftoff of 0 and the other state (blue) has a liftoff of 4.  It looks like the counts in the state depicted by the black curve is shooting up and by the end of the sequence almost double the count than the state represented by the blue curve.

```{r echo=longform}
plot(0:max.day.numeric, richfu(0:max.day.numeric, 1, 1.23 , .000555 , 1)*19500000/100,type="l",xlab="days",ylab="counts",main="Hypothetical plot of counts for two states\n with different liftoffs")
lines(0:max.day.numeric, richfu(0:max.day.numeric, 4, 1.23 , .000555 , 1)*19500000/100, col="blue")
```

To illustrate how time locking can show structure, let me use the previous plot, where the two states look to have different patterns.  Time locking the two states to the first day when they each have nonzero cases shows that both states have identical trajectories. It is just that one state started earlier than the other state and so the patterns in the previous plot look (dramatically) different, and just by a delay of 4 days!  Much like singing ["Row, row, row your boat" in rounds](https://www.youtube.com/watch?v=2d_GLxa4_bg)---voices sing the same melody but start at different times creating a harmonious whole.



```{r echo=longform}
#very kludgy code to impose time locking
plot(c(0:max.day.numeric)[-1], richfu(c(0:max.day.numeric)[-1], 1, 1.23 , .000555 , 1)*19500000/100,type="l",xlab="days",ylab="counts",
     main="Hypothetical plot of counts for two states \n with different liftoffs but time locked")
lines(c(0:max.day.numeric)[-1*c(1:6)]-5, richfu(c(0:max.day.numeric)[-1*c(1:6)]-2, 4, 1.23 , .000555 , 1)*19500000/100, col="blue")
```

```{asis echo=longform}
There are better ways to time lock using specific tools written for time series data, but those involve much more explanation that I want to go into just for illustrating the usefulness of time locking time series.   For additional information see R packages written for time series data such as functional data analysis R packages.
```

A good [example](https://www.ft.com/coronavirus-latest) of controlling for different liftoff patterns is by John Burn-Murdoch of the Financial Times. These visualization are unique from the ones I have seen on the web in that their curves are not plotted on calendar time but in terms of when the country hit X cases (sometimes X is 3 or 10 depending on the graph). This normalizes the onset and allows us to compare across units that had different starting points. A common phrase for this is "time locking" to a particular event, in this case time locking to when a country hit, say, 10 cases.

There is a sense in which we didn't have to worry about liftoff in the linear mixed model on the log scale we ran in the previous subsection because the intercept parameter essentially took care of that.  We could complicate the exponential model by including a random effect intercept, but I'll keep things simple in these notes.

### Estimating the exponential with the nonlinear mixed model

Let's estimate an exponential growth model using a nonlinear mixed model regression. This mimics the LMER model we ran earlier in these notes but rather than transforming with a log to convert the data into straight lines we model the curvature directly with an exponential function. To simplify things I constrain the liftoff parameter to 0 and the multiplier b1 to 1.  To estimate all four parameters simultaneously we need to move to a different R package [saemix](https://cran.r-project.org/web/packages/saemix/saemix.pdf) that uses a stochastic EM algorithm to estimate this nonlinear mixed model because nlme can't handle very complex models like this one without doing more ground work in setting up a self-starting function that has information about the gradient of the function.  But, if you do that be careful that the function I'm proposing can even be estimated. There may be a redundancy with the rate and the b1 parameters (related to the base of the exponent that I described earlier in the [Exponential Growth Background](#expoback) subsection when moving to different bases like base 10 or $\exp$).

For these estimations I'm using percent of positive cases in the state (count/population) rather than count.

```{r message=F, warning=F, results=ifelse(longform, "markup", "hide"), echo=longform}
out <- nlme(percent ~ richfu(day.numeric, l=0, rate,y,b1=1), fixed = rate+y~1,
            random =  rate+y ~1, data=na.omit(allstates.long.new),start=c( rate=1.2,y=.01), verbose=T, control=nlmeControl(msMaxIter=500,opt="nlminb",pnlsTol=.01,msVerbose=T,minScale=.01) )
```

```{r message=F, warning=F, echo=longform}
out.exponential <- out

if (is_html_output())
summary(out.exponential)
if (is_latex_output())
  texreg(out.exponential, booktabs=T, dcolumn = T)

#intervals(out.exponential, which="fixed")
intervals(out.exponential)

#idea for boostrapping to get predicted interval but be careful about the time aspect; maybe better to simulate data 
#https://stackoverflow.com/questions/55552080/how-to-obtain-confidence-intervals-for-non-linear-mixed-model-logistic-growth-c
```

Checking residuals for assumptions.

```{r echo=longform}
plot(fitted(out.exponential), resid(out.exponential), main="New York in blue")
#suspect weird points are new york; so put NY in blue points and connected dots with a curve
points(fitted(out.exponential)[names(fitted(out.exponential))=="New York"],resid(out.exponential)[names(resid(out.exponential))=="New York"],col="blue")
lines(fitted(out.exponential)[names(fitted(out.exponential))=="New York"],resid(out.exponential)[names(resid(out.exponential))=="New York"],col="blue")

#let's drop outliers and look at the resid plot (zoom in for fits < .08 and abs(resid) < .01)
#this helps us see the majority of the residual plot
plot(fitted(out.exponential)[
  fitted(out.exponential)<.08 & abs(resid(out.exponential)) < .01], 
  resid(out.exponential)[fitted(out.exponential)<.08 & abs(resid(out.exponential)) < .01], 
  xlab="fits < .08", ylab="abs(resid) < .01", main="zoom in ")

qqnorm(resid(out.exponential))
qqline(resid(out.exponential))
```

Some  issues with residuals as seen in the residual scatterplot against the fitted values and the QQ plot suggesting longer tails than expected if residuals were distributed normally.  The first plot above shows a relatively strange pattern in the residuals when plotted against the fitted values. The blue points correspond to New York (I connected them with line segments to they stand out).  The second zooms in and suggests there still is some structure in the residuals the model is not picking up.  The third plot is a QQ plot suggest extremely long tails.

Let's plot  the fitted values of each state.  These are the predicted exponential curves according to the model. Note that these are model estimates, not a plot of the actual data.

```{r fig.cap="Exponential Function Estimated through Nonlinear Mixed Model Regression", echo=longform}
predmatnonlin <- data.frame(day.numeric=rep(c(0:max.day.numeric),50),state.abb=rep(rownames(coef(out.exponential)),each=max.day.numeric+1))
predmatnonlin$prediction <- predict(out.exponential, list(day.numeric=predmatnonlin$day.numeric, state.abb=predmatnonlin$state.abb),level=1)
predmatnonlin.exponential <- predmatnonlin
nonlinreg <- ggplot(predmatnonlin, aes(day.numeric,prediction,group=state.abb, color=state.abb )) +  geom_line()   
nonlinreg  
```


```{r echo=longform}
nydf <- predmatnonlin[predmatnonlin$state.abb=="New York",]

nydf$percent <- allstates.long$percent[allstates.long$state.abb=="New York"]
nydf.exponential <- nydf
ggplot(nydf, aes(day.numeric,prediction,group=state.abb, color=state.abb )) + geom_line() + geom_point(aes(x=day.numeric, y=percent), col="blue") + ggtitle("New York: Actual percentage in blue; exponential growth in red")
```

Consistent to what we observed in the log transformed data, the points do not seem to follow an exponential curve (New York's poor fit being one example).  We will move to a different form that may be more appropriate for these data.

<!-- Here one could run the same code above I used in the linear mixed model case on the log scale to produce tables of predicted counts, their confidence intervals, and the forecast plots with the confidence bands. This information could be compared with the previous ones to help us evaluate the models.  Some thought needs to go into writing code to simulate the confidence bands around the predictions from the nonlinear model. -->

<!-- I'll compare the predicted curve for New York five days out from 3/23 using this nonlinear mixed model regression to the earlier linear mixed model regression on the log scale, by superimposing the predicted curve in green from the nonlinear model on the plot for  the log linear model. -->

```{r eval=F, echo=F}
#need to rewrite this to incldue predictioninteval for nlme (don't use temp from log linear version)
#go days.ahead out
predmatnonlin <- data.frame(day.numeric=rep(c(0:(14+days.ahead)),50),state.abb=rep(rownames(coef(out)),each=15+days.ahead))
predmatnonlin$prediction <- predict(out, list(day.numeric=predmatnonlin$day.numeric, state.abb=predmatnonlin$state.abb),level=1)

#focus just on New York for simplicity
datapl <- data.frame(predmatnonlin[predmatnonlin$state.abb=="New York", ])

#convert the fits back to counts to match the plot used in the log linear 
datapl$counts <- datapl$prediction * c(state.population[state.population$NAME=="New York","value"])$value / 100

y = allstates.long.new[allstates.long.new$state.abb == "New York","count"]
actualny <- data.frame(y=y, x=0:(length(y)-1))

temp %>%   ggplot(aes(x=day.numeric, y=fit)) + geom_point(data=subset(temp,day.numeric<=max.day.numeric) )+ geom_ribbon(data=subset(temp,day.numeric>max.day.numeric), aes(y=fit, ymin=lwr,ymax=upr), alpha=.2,fill="red") + geom_line() + geom_linerange(data=subset(temp,day.numeric<=max.day.numeric),aes(y=fit, ymin=lwr,ymax=upr)) + ylab("Counts") + geom_point(data=manualcounts, aes(x=day.numeric, y=fit),color="blue",shape=2) + geom_line(data=datapl, aes(x=day.numeric, y=counts),color="green") + geom_point(data=datapl, aes(x=day.numeric, y=counts),color="green") + geom_point(data=datapl, aes(x=day.numeric, y=counts),color="green") + geom_point(data=actualny, aes(x=x,y=y), color="violet")

```


## Logistic (Nonlinear) Regression 

Let's switch to a logistic growth model (which is not the same as logistic regression for binary data).  The nlme package has some additional routines  to simplify the fitting of logistic growth models (e.g., SSlogis), which make it easier to set up starting values. I'll avoid that so as to be more transparent with the code I'm writing.  In other work I've seen more efficient estimation (and less fussiness around starting values) with the stochastic EM approach to fitting nonlinear regression models such as in the package saemix.

The nlme() command has a difficult time with the three parameters of the logistic growth model. I did a little bit of tinkering with starting values and the three parameters.  I was able to get a reasonable fit with fitting three parameters of the logistic growth model and having two of them be random effects (i.e., they are allowed to vary by state).  A more careful analysis would need to be done to make this publishable, including exploring other parameterizations of logistic growth that may behave better with nlme. I give hints in the commented out sections of the R code.

The logistic growth model starts off being close to an exponential at first but eventually tapers off. Many biological systems follow a logistic growth function. Here is a link to a relatively simple explanation of [logistic growth](https://courses.lumenlearning.com/ivytech-collegealgebra/chapter/use-logistic-growth-models/).

This is the form of logistic growth I'm using

$$
\begin{aligned}
count &= \frac{asymptote}{1 + \exp{\beta_1(day.mid - day)}}
\end{aligned}
$$
The asymptote parameter controls the maximum value the function reaches, the day.mid parameter is both the day at which the function is half way to the asymptote and is also the inflection point and $\beta_1$ is a scaling parameter defined to be the distance between the inflection point day.mid and point at which the count is 73% of the asymptote (basically, the difference between day.mid and the day corresponding to the count asymptote*$\frac{1}{1+exp(-1)}$, see Pinhero & Bates).

```{r messages=F, warnings=F, echo=longform}

richfu.logistic <- function(d,l,y,b1){
l/(1+y*exp(b1*d))
}

#example logistic growth model
plot(0:max.day.numeric, richfu.logistic(0:max.day.numeric, 3880, 133.45, -.332),type="l",xlab="days",
  ylab="counts",main="Hypothetical plot of counts following a logistic growth function")
```

Let's fit the logistic function to these data.  Note that I do not use state population size in this model.  
```{asis echo=longform}
I could incorporate it by including a weight parameter (weights=varIdent(form= ~1|state.abb)), which allows each state to have its own residual variance, or change the dependent variable from raw counts to, say, per capita counts.  I'll leave that as an exercise to see whether it changes the results. If you go the unequal residual variance route, you can use the anova command to compare the fits, which is essentially an increment in fit test comparing the reduced model (e.g., one residual variance for all states) to a full model (e.g., each state has its own residual variance). The command would be anova(reduced, full) where you replace reduced and full with the name of the object you saved the nlme (e.g., reduced = out.logistic in the example below).
```

```{r messages=F, warnings=F, results="hide", echo=longform}
out <- nlme(count ~ richfu.logistic(day.numeric, l,y,b1), fixed = l+y+b1~1,
            random =  l+y~1, data=na.omit(allstates.long.new),start=c( l=17500,y=170,b1=-.15), verbose=T, control=nlmeControl(msMaxIter=1000,opt="nlminb",pnlsTol=.01,msVerbose=T,minScale=.01, maxIter=1000, eval.mas=1000) )
```

```{r  messages=F, warnings=F, echo=longform}
#AIC(out)
out.logistic <- out

if (is_html_output())
summary(out.logistic)

if (is_latex_output())
  texreg(out.logistic, booktabs=T, dcolumn = T)

#confidence intervals of the parameters
intervals(out.logistic)

#Checking residuals for assumptions
plot(fitted(out.logistic), resid(out.logistic), main="New York in blue")
#suspect weird points are new york; so put NY in blue points and connected dots with a curve
points(fitted(out.logistic)[names(fitted(out.logistic))=="New York"],resid(out.logistic)[names(resid(out.logistic))=="New York"],col="blue")
lines(fitted(out.logistic)[names(fitted(out.logistic))=="New York"],resid(out.logistic)[names(resid(out.logistic))=="New York"],col="blue")

#let's drop outliers and look at the resid plot (zoom in for fits < .08 and abs(resid) < .01)
#this helps us see the majority of the residual plot
plot(fitted(out.logistic)[
  fitted(out.logistic)<20000 & abs(resid(out.logistic)) < 1000], 
  resid(out.logistic)[fitted(out.logistic)<20000 & abs(resid(out.logistic)) < 1000], 
  xlab="fits < 20000", ylab="abs(resid) < 1000", main="zoom in ")


qqnorm(resid(out.logistic))
qqline(resid(out.logistic))

#autocorrelation, set lag to 6 given small data set
ACF(out.logistic, maxLag=6)
plot(ACF(out.logistic, maxLag=6))

#example of another version that had higher AIC so I discarded this model
#out2 <- nlme(count ~ richfu.logistic(day.numeric, l,y,b1), fixed = l+y+b1~1,
#            random =  l+b1~1, data=na.omit(allstates.long.new),start=c( l=15000,y=800,b1=-.65), verbose=T, #control=nlmeControl(msMaxIter=1000,opt="nlminb",pnlsTol=.02,msVerbose=T,minScale=.02, maxIter=1000) )
#AIC(out2)

#example of a different parameterization of logistic growth (previous y = 1 and allowing shift by subracting by parameter)
#starts <- getInitial(count~ SSlogis(day.numeric, Asym=1, xmid=1, scal=1), data=allstates.long.new)
#out3 <- nlme(count ~ SSlogis(day.numeric, Asym, xmid,scal), fixed =  Asym + xmid + scal  ~1,
#            random = xmid + Asym ~ 1, data=na.omit(allstates.long.new),start=c(Asym=starts[1], xmid=starts[2],scal=starts[3]), verbose=T, control=nlmeControl(msMaxIter=500))
#AIC(out3)
```

Same three residual plots as before, with slight improvement as seen in the zoomed in version and slightly less extreme tails. Not completely satisfying.  I also include the autocorrelation function plot.


```{r messages=F, warning=F, eval=F, echo=longform}
#stopped working 4/4/20 (likely a few outliers in the data points so need to investigate)
#For illustration purposes, here is another way of fitting this model using the a different package lme4.  
#I'll only provide a summary and a residual plot.
# Asym/(1+exp((xmid-input)/scal))
out.logistic.nlmer <- nlmer(percent ~ SSlogis(day.numeric, l, y,b1) ~ l + y + b1| state.abb, data=na.omit(allstates.long.new), start=c( l=5300, y=20, b1=3.5), 
                            control=nlmerControl(tolPwrss=.05, optCtrl=list(maxiter=500)))
summary(out.logistic.nlmer)

plot(0:26, .1157/(1+exp((19.2 - 0:26)/3.5)), type="l")
dp <- data.frame(na.omit(allstates.long.new), res=resid(out.logistic.nlmer), fitted=fitted(out.logistic.nlmer))
diagplot <- ggplot(dp,aes(x=fitted,y=res,colour=state.abb))+geom_point()+
#  geom_smooth(aes(group=1),colour="black",method="loess")+
  geom_hline(yintercept=0,lty=2) + scale_x_continuous(limits=c(0, .1))
diagplot


```


Below is the plot of predicted curves according to the model. Notice that the fits for some states begin to taper off according to this model, but for the previous exponential growth model all the states showed exponential.  One could examine these differences to see which model performs better in predictive accuracy (i.e., see if the actual data for the state tapers off or continues to grow exponentially). Such patterns are testable and the ideal way to compare models and decide which one provides a better representation of the data.



```{r fig.cap="Logistic Growth Model", echo=longform}
predmatnonlin <- data.frame(day.numeric=rep(c(0:max.day.numeric),50),state.abb=rep(rownames(coef(out.logistic)),each=max.day.numeric+1))
predmatnonlin$prediction <- predict(out.logistic, list(day.numeric=predmatnonlin$day.numeric, state.abb=predmatnonlin$state.abb),level=1)
predmatnonlin.logistic <- predmatnonlin
nonlinreg.logistic <- ggplot(predmatnonlin, aes(day.numeric,prediction,group=state.abb, color=state.abb )) +  geom_line()   
nonlinreg.logistic
```

Here is the predicted curve for New York. We know the residuals suggest the model is 'off' for NY (the residual plot systematically under and over predicting), but this plots suggests a logistic is a reasonable fit, and most of the part that is off is that it didn't capture the liftoff and sharp increase once the curve started to increase.

```{r echo=longform}
nydf <- predmatnonlin[predmatnonlin$state.abb=="New York",]

nydf$count <- allstates.long$count[allstates.long$state.abb=="New York"]
nydf.logistic <- nydf
ggplot(nydf, aes(day.numeric,prediction,group=state.abb, color=state.abb )) + geom_line() + geom_point(aes(x=day.numeric, y=count), col="blue") + ggtitle("New York: Actual counts in blue; logistic growth in red")
```

Another way to compare across models is to use an index such as the AIC, which is a measure based on information theory and akin to R^2 adjusted that can be used to compare regression models that vary in the number of parameters. One needs to be careful when using AIC across models where the dependent variable is on different scales as in these notes. For example, my log transform of percentage cases (count/population) in the linear mixed model is on a different scale than the exponential nonlinear regression model I computed above.  R provides such information-based criteria measures such as AIC() and an alternative called BIC(). Here I give AIC and BIC for the logistic growth models. When comparing models on comparable dependent variables the model with the lower AIC (or BIC) value is selected as the best model.

```{r echo=longform}
AIC(out.logistic)
BIC(out.logistic)
```


Another consideration is that the residuals may not be independent given that these are time series data. Yestersday's residual may be correlated with today's residual.  Here I add a correlation structure to the nlme command using an autocorrelation lag of 1 (AR p =1) and a moving average lag of 1 (MA q = 1).  The ARMA implementation in nlme, though, operates completely on the residuals. So, the AR refers to the previous residual correlation and the MA refers to a set of independent and identically distributed noise terms apart from the residuals (see Pinhero & Bates, 2000, as this is not standard).  You can think of the MA noise parameters are "factors" that are added to produce an additional source of correlation across residuals, much like a common factor would, but separate from the serial correlation of resisuals.  I also use the ACF() command to display the autocorrelation function.  One could also try a heterogeneous residual variance model (the weight argument described above) along with the correlated error.  Much more diagnostics would be needed to settle on the appropriate correlational structure for the error term plus the usual checks on the residuals to ensure we are properly meeting the assumptions.

```{r messages=F, warning=F, results="hide", echo=longform}
out <- nlme(count ~ richfu.logistic(day.numeric, l,y,b1), fixed = l+y+b1~1,
            random =  l+y~1, data=na.omit(allstates.long.new),start=c( l=14500,y=232,b1=-.2), verbose=T, correlation=corARMA(form=~1,p=1,q=1), control=nlmeControl(msMaxIter=1000,opt="nlminb",pnlsTol=.02,msVerbose=T,minScale=.02, maxIter=1000, eval.mas=1000) )
```

```{r messages=F, warning=F, echo=longform}
out.logistic2 <- out
#ACF(out.logistic2, maxLag=6)
#plot(ACF(out.logistic2, maxLag=6))

#Checking residuals for assumptions
#plot(fitted(out.logistic2), resid(out.logistic2), main="New York in blue")
#suspect weird points are new york; so put NY in blue points and connected dots with a curve
#points(fitted(out.logistic2)[names(fitted(out.logistic2))=="New York"],resid(out.logistic2)[names(resid(out.logistic2))=="New York"],col="blue")
#lines(fitted(out.logistic2)[names(fitted(out.logistic2))=="New York"],resid(out.logistic2)[names(resid(out.logistic2))=="New York"],col="blue")

#qqnorm(resid(out.logistic2))
#qqline(resid(out.logistic2))
```

Here is a test of the two logistic growth models (with and without correlated error structure).
```{r messages=F, warning=F}
#test the reduced (no correlation) to the full (with ARMA(1,1) correlational structure)
anova(out.logistic, out.logistic2)
```


<!-- For completeness, the plot of predicted trajectories for the logistic growth model with correlated error structure, followed by New York for comparison with the earlier logistic growth model with independent residuals. -->

```{r messages=F, warning=F, echo=longform, eval=F}
#print curves and ny data
predmatnonlin2 <- data.frame(day.numeric=rep(c(0:max.day.numeric),50),state.abb=rep(rownames(coef(out.logistic2)),each=max.day.numeric+1))
predmatnonlin2$prediction <- predict(out.logistic2, list(day.numeric=predmatnonlin2$day.numeric, state.abb=predmatnonlin2$state.abb),level=1)
nonlinreg.logistic2 <- ggplot(predmatnonlin2, aes(day.numeric,prediction,group=state.abb, color=state.abb )) +  geom_line()   
nonlinreg.logistic2
nydf2 <- predmatnonlin2[predmatnonlin2$state.abb=="New York",]

nydf2$count <- allstates.long$count[allstates.long$state.abb=="New York"]
ggplot(nydf2, aes(day.numeric,prediction,group=state.abb, color=state.abb )) + geom_line() + geom_point(aes(x=day.numeric, y=count), col="blue") + ggtitle("New York: Actual counts in blue; logistic growth in red")
```

### Additional Predictors: Example with State-Level Poverty Rate

Now that we have a reasonable model, let's see how we can include predictors to the model.  One way to include predictors is to see if the parameters of the logistic growth regression are influenced by the predictor, e.g. do the state rate parameters vary according to the predictor.   We can accomplish this by, say, having the rate for each state be a linear function of the predictor, as in rate = $\beta_0$ + $\beta_1$ predictor + $\epsilon$.  Our current model is merely rate = $\beta_0$ + $\epsilon$ where the intercept is the fixed effect (same value for each state) and the "$\epsilon$" is the random effect (a term associated with each state).


For illustration, I examine whether the poverty rate in each state is associated with the coefficient of the multiplier y we estimated in the logistic growth model. 

```{r echo=longform}
par(pty="s")
plot(allstates$povertyperc, coef(out.logistic)[,2],xlab="State Poverty Percentage", ylab="random effect parameter y in logistic growth")
abline(coef(lm(coef(out.logistic)[,2] ~ allstates$povertyperc)))
par(pty="m")
```

 It looks like a slight negative association is present though the data are noisy. In principle, I could test this regression slope through a regular regression but it is better to do it in the context of the nonlinear logistic growth regression model. The reason is that a linear regression using the coefficients from the logistic growth as though they were an observed variable doesn't take into account the error structure of these parameters---the parameters aren't observed data but they are  estimates from another model.  By including the poverty predictor simultaneously with the logistic growth model we model the uncertainty much better. It would be even better to run this as a Bayesian model, which I'll add in a later version using the brm package.
 
 <!-- https://cran.r-project.org/web/packages/brms/vignettes/brms_multilevel.pdf -->
 
```{r messages=F, warning=F, results="hide", echo=longform}
out <- nlme(count ~ richfu.logistic(day.numeric, l,y+b2*povertyperc,b1), fixed = l+y+b1 + b2~1,
            random =  l+y~1, data=na.omit(allstates.long.new),start=c( l=17500,y=200,b1=-.15, b2=-4.5), verbose=T,  control=nlmeControl(msMaxIter=1000,opt="nlminb",pnlsTol=.02,msVerbose=T,minScale=.02, maxIter=1000, eval.mas=1000) )
```


```{r echo=longform, results=ifelse(longform, "markup", "hide")}
out.logistic.poverty <- out

if (is_html_output())
summary(out.logistic.poverty)

if (is_latex_output())
  texreg(out.logistic.poverty, booktabs=T, dcolumn = T)

intervals(out.logistic.poverty)
```

The $\beta$ associated with poverty is not statistically significant in this example so we do not have sufficient justification to claim that the state variability in the y parameter of this logistic growth model is related to the state's poverty level.  Anyways, this example illustrates how one goes about adding predictors to such a nonlinear model.  One could use more complicated predictors, even test differences across groups on parameter values (e.g., do states with democratic governors have different parameter values than states with republican governors); one follows the same principles for testing predictors and groups in a standard regression model. While the presentation has focused on associations, the overall framework can be extended to strengthen inference using various methods including instrumental variables (e.g., [Carrol et al, 2004](https://www.stat.tamu.edu/~carroll/ftp/2004.papers.directory/iv_jasa_2004.pdf)) and newer approaches (e.g., [Runge et al, 2019](https://advances.sciencemag.org/content/5/11/eaau4996)), though much theoretical work remains to be done to move these nonlinear mixed models from mere description to stronger inferences by allowing more direct tests of possible alternative explanations.  For an introduction to some basic modern methods in causal inference see [Bauer and Cohen](https://bookdown.org/paul/applied-causal-analysis/).

More diagnostic checking is needed, however, to have confidence in this logistic growth model, with or without predictors. The two random effect parameters in the nonlinear mixed logistic growth model are assumed to follow a bivariate normal distribution, which does not seem to be the case.  Here is a 2D density plot (I know, a lot to ask for when there are only 50 pairs of points, one for each state), separate qqplots and boxplots for the asymptote and y parameters.  The qqplot for the asymptote parameter l has a long tail, mostly due to New York and New Jersey; the one for the y parameter looks symmetric. 

```{r echo=longform}
ggplot(coef(out.logistic), aes(x=l, y=y)) + stat_density_2d(aes(fill = ..level..), geom = "polygon", colour="white") + xlab("l = asymptote")

par(mfcol=c(1,2))
qqnorm(coef(out.logistic)[,1],main="qqplot for asymptote parameter l")
qqline(coef(out.logistic)[,1])
qqnorm(coef(out.logistic)[,2],main="qqplot for y")
qqline(coef(out.logistic)[,2])

boxplot(coef(out.logistic)[,1], ylab="asymptote l parameter")
boxplot(coef(out.logistic)[,2], ylab="y parameter")

par(mfcol=c(1,1))
```


We'll have to use more sophisticated approaches to model these data given that assuming a nonlinear model with normally distributed parameters may not be tenable for these data.  

The overall point is that testing whether the data fit an exponential growth model,  a logistic growth model, or various other plausible growth trajectories is not merely a modeling fitting exercise. There is much diagnostic testing one needs to do in order to figure out in what ways is a model working well and where it is messing up.  The model selection game is more complex than merely selecting the model that has the best fit value on a standard metric such as AIC or BIC or predictive accuracy.  To me, there is deeper insight that emerges from understanding the type of growth that is underlying the processes under investigation. We are in a better position to understand how to intervene if we understand the growth process that underlies a data set.  Just because a model achieves relatively good performance on measures of fit does not imply that the model is following the same mechanisms of the processes that generated the actual data.  This is where the diagnostic tests come in to help us diagnose such situations.  

To get deeper in such process models of growth, we need to develop some additional machinery including differential equations, which are equations that govern changes over time.  I'll briefly touch on these models in the next chapter.


## Splines and Smoothers

I don't recommend going the route of splines and smoothers in this context.  While they can provide great fit to the data and can be used for interpolation (e.g., if we missed a day of data collection, we could interpolate), splines and smoothers are not helpful at informing one of the underlying process nor in forecasting (extrapolating)  from one's data, which is what we want to be able to do in this case.  For illustration, and to show that I too can get curves that approximate the data points closely, I ran a mixed model spline regression where the knots are treated as a random effect and show the plots just for New York and Michigan for brevity. One would need to do all the usual diagnostics for this mixed model as well.

```{r message=F, warning=F, echo=longform, results=ifelse(longform, "markup", "hide")}
#linear mixed model using splines on the count data
out.sp <- lmer(count ~ (bs(day.numeric, intercept=T, degree=5) | state.abb), data=na.omit(allstates.long))

#diagnostics omitted for brevity
#plot(out.sp)
#qqnorm(resid(out.sp))
#qqline(resid(out.sp))
out.temp <-na.omit(allstates.long)
out.temp$fit <- fitted(out.sp)
subset(out.temp,state.abb %in% c("New York","Michigan")) %>% ggplot(aes(x=day.numeric,y=fit,group=state.abb,color=state.abb)) + geom_line() + geom_point(aes(x=day.numeric,y=count), color="black") + ggtitle("Black points are actual data; curves are spline regression fits")
```


<!-- I'll compare the predicted curve for New York five days out from 3/23 using this nonlinear mixed model regression to the earlier linear mixed model regression on the log scale, by superimposing the predicted curve in green from the nonlinear model on the plot for  the log linear model. -->

```{r eval=F, echo=F}
#once nlme is fixed for exponential, then fix this segment
#go days.ahead out
predmatnonlin <- data.frame(day.numeric=rep(c(0:(14+days.ahead)),50),state.abb=rep(rownames(coef(out)),each=15+days.ahead))
predmatnonlin$prediction <- predict(out, list(day.numeric=predmatnonlin$day.numeric, state.abb=predmatnonlin$state.abb),level=1)

#focus just on New York for simplicity
datapl <- data.frame(predmatnonlin[predmatnonlin$state.abb=="New York", ])

#convert the fits back to counts to match the plot used in the log linear 
datapl$counts <- datapl$prediction * c(state.population[state.population$NAME=="New York","value"])$value / 100

y <- allstates.long.new[allstates.long.new$state.abb == "New York","count"]
actualny <- data.frame(y=y, x=0:(length(y)-1))

temp %>%   ggplot(aes(x=day.numeric, y=fit)) + geom_point(data=subset(temp,day.numeric<=max.day.numeric) )+ geom_ribbon(data=subset(temp,day.numeric>max.day.numeric), aes(y=fit, ymin=lwr,ymax=upr), alpha=.2,fill="red") + geom_line() + geom_linerange(data=subset(temp,day.numeric<=max.day.numeric),aes(y=fit, ymin=lwr,ymax=upr)) + ylab("Counts") + geom_point(data=manualcounts, aes(x=day.numeric, y=fit),color="blue",shape=2) + geom_line(data=datapl, aes(x=day.numeric, y=counts),color="green") + geom_point(data=datapl, aes(x=day.numeric, y=counts),color="green") + geom_point(data=datapl, aes(x=day.numeric, y=counts),color="green") + geom_point(data=actualny, aes(x=x,y=y), color="violet")

```


## Bayesian Estimation of Nonlinear Regression: Adding value to prediction and uncertainty

Pending: illustrate use of Bayesian nonlinear regression modeling to estimate prediction uncertainty, allow for more general models than the standard regression approaches described earlier in this chapter and highlight some of the issues with the standard assumptions for random effects modeling that can be examined more easily in the Bayesian context.

## Dynamic Systems Modeling

Pending:  this section will include state-space models and dynamic regressions with the R packages pomp and dynr.  I'll present a basic intro to these models here and then return to them in Chapter \@ref(process) when discussion process models.

## Basic Machine Learning Examples

In my opinion, while impressive progress has been made in text analysis and image analysis, the machine learning literature has lagged behind where it needs to be on development of methods that address multiple time series (e.g., times series by state, country or county).  Here I review one approach, an implementation of a recurrent neural network (RNN) that I used for the data from  New York state. We need to use an RNN, a specialized version of a neural network, because we have time series data. [Here](https://www.kaggle.com/rtatman/beginner-s-intro-to-rnn-s-in-r) is one description, [another description](http://karpathy.github.io/2015/05/21/rnn-effectiveness//) and a [third description by UM's Ivo Dinov](http://www.socr.umich.edu/people/dinov/courses/DSPA_notes/18_BigLongitudinalDataAnalysis_RNN.html) of RNNs. The R package I will illustrate is rnn and it is a basic implementation of a recurrent neural network. There are much better implementations, such as the keras package and I'll explore that later once I make more progress on other parts of these notes.  For now, we'll use the very simplified rnn package in R.

The underlying common element of a neural network is that they take inputs, weight them, sum and add a "bias" term.  This is similar to regression in weighting predictors by betas, sum the products and add and intercept. The weighted sum is then converted into a number between 0 and 1, using the logistic form similar to logistic regression that converts the model on the real number line into the scale of the dependent variable that is a probability between 0 and 1.  This can be done repeatedly to create multiple units, or different sets of weighted sums, in a manner similar to principal components analysis (PCA) where the observed variables are weighted and summed to produce components.  It is possible to create additional layers with new outputs based on these weighted sums (so, weighted sums of nonlinear transformations of weighted sums). The the final layer of units  is then mapped onto the outcome variable.

Here is a graphical representation of what I mean.  This example has four inputs (i.e., four variables or features), one hidden layer with 8 units, and one output layer that is a single number prediction (like our counts of positive tests).  The hidden layer is a set of 8 units. The two B terms are the "bias" terms that feed into the weighted sums.  The thick black lines refer to higher positive weights and the thicker gray refer to smaller negative.   Example adapted from the tutorial in the NeuralNetTools R package.

```{r echo=F, results="hide"}
library(NeuralNetTools)
library(nnet)

data(neuraldat)

neuraldat$X4 <- neuraldat$X1* neuraldat$X2

mod <- nnet(Y1 ~ X1 + X2 + X3 + X4, data = neuraldat, size = 8)
```

```{r echo=F}
# plot
par(mar = numeric(4))
plotnet(mod)
```

It is common to model time series using the counts for each day (such as with the [incidence plot](#incidenceplots) we saw in the Section \@ref(incidenceplots)) rather than the cumulative counts we have been using throughout most of these notes. I'll follow that convention here. Here is the incident plot for New York State.

```{r echo=longform}
data.temp <- allstates.long %>% filter(state.abb == "New York")
data.temp$daily <- c(data.temp$count[1],diff(data.temp$count))

ggplot( data.temp, aes(x=day,y=daily)) +  geom_bar(stat="identity") +
    scale_x_date(date_breaks="1 week", date_labels = "%d %b") +
    labs(y="Daily incremental incidence ",
                                   title="Positive US Covid-19 Cases New York")  +
    theme(legend.position = "none",
          strip.text.y = element_text(size=11))
```

### Simple RNN example

The following involves a relatively large chunk of code. Other implementations of deep learning networks require even more code.  The basic idea is that we put all our data into one data frame, then partition it into testing data and training data. We want to be sure we partition early so as not to contaminate the testing data with any information that is in the testing data. For example, if we compute a mean we may inadvertently compute a mean for all the data (testing and training), and then if we use that mean in training we have contaminated the training data with information from the testing data.  

The only information I have to work with so far is the number days and the previously observed daily counts; that is, as of this point in these notes we don't have access to other possible predictors of a state's daily count.  Here, "previously observed" means if I'm at time t, then any data prior to t is "previous" data that I have but any data after t has not occurred. In the case of data I'm downloading, t may be, say, 4/10/20 so from that perspective any data after 4/10/20 cannot be used because as of that time point t I don't know of data after t.  Part of the machine learning algorithm works by varying t to different days, repeatedly feeding those sequences of data to the model and trying to learn the patterns by figuring out what it is getting wrong, making adjustments in the model's parameters (the weights and bias terms) and  making new predictions, and iterating that process.

The rest of the code is creating daily counts and several features to use as predictors. We don't have a rich set of predictors set up yet so I created some from the day variable. I will use day, day$^2$ and day$^3$ (of course, mean centering day before computing the higher order terms). I created a predictor of how much daily change there was (e.g., the count at t minus the count at t - 1). This serves as a slope estimate of change, which is a first order difference because I'm working in discrete time (days); technically, it is a secant rather than the slope (which would correspond to the tangent).  I also created a "difference of difference" (or second order difference analogous to acceleration, or how the change is changing) to measure how much the slope is changing from day to day. In other words how different is the change today from the change we saw yesterday.  This process of creating new variables from existing variables is called "feature engineering."

This model has 6 inputs:  day, day$^2$, day$^3$, count, count difference, and count second order difference. I currently have just have a few units in a single hidden layer, and one unit in the output layer.  Of course, to make this more interesting I could add many more predictors including state-level and daily information (e.g., measures of compliance to social distancing measures; weather, which could affect people's behavior going outside; changes in a state's social distancing policy; etc.).

```{asis echo=longform}
So, overall the long section of code is because we have to do some work to put the data in the right format for machine learning to do its magic.  
```



In the figure below the red dots are the actual daily cases in New York.  The green curve is what the machine learning model learned. I intentionally over trained the model. The last day shows the prediction by the machine learning model in solid blue and the actual in solid red.  

```{r cache=F, warning=F, message=F, echo=longform, results=ifelse(longform, "markup", "hide")}
#set.seed(03302020)
y = allstates.long.new[allstates.long.new$state.abb == "New York","count"]
actualny <- data.frame(y=y, x=0:(length(y)-1))

data <- data.frame(days= actualny$x[-1], counts= actualny$y[-1])

#partition train and test data; good to do that first so that test data doesn't creep into training
#this could happen accidentally say when computing a mean
#for this classroom example the test set is the last day, so I'm testing by a prediction one day out
data_train <- data[-nrow(data),]
data_test <- data[nrow(data),]

#diff computes first order differences so need to add day 1 at the begining of the series
time.series.day.counts <- ts(c(173,diff(data_train$counts)))
time.series.day.counts

#first order difference
first.diff  <- diff(time.series.day.counts)
first.diff
#second order difference
second.diff  <- diff(first.diff)
second.diff

#put into data matrix and pad differences with NA 
data.mat <- data.frame(days.numeric=data_train$days-mean(data_train$days), first=c(NA,first.diff), second=c(NA,NA,second.diff))
#create day sq and day cube with centering
data.mat$daysq <- (data.mat$days.numeric-mean(data.mat$days.numeric))^2
data.mat$daycube <- (data.mat$days.numeric-mean(data.mat$days.numeric))^3
data.mat$previous.day.count <- c(NA, time.series.day.counts[-length(time.series.day.counts)])

#but to make future prediction is sensible I can't compute first and second order difference knowing the future count I'm predicting
#so move the 1st and 2nd order diff down one row and pad with NAs; save the last count I'm dropping here because I do know it when 
#making prediction to the training set (previous.day.count is already moved up)
save.last.row.data.mat <- data.mat[nrow(data.mat),]
data.mat[,2:3] <- cbind(c(NA, data.mat[1:(nrow(data.mat)-1),2]), c(NA, data.mat[1:(nrow(data.mat)-1),3]))

#drop first three rows to avoid missing data
data.mat <- data.mat[-c(1:3),]

#define scaling function for scaling 0-1
sc <- function(x){(x-min(x,na.rm=T))/(max(x,na.rm=T)-min(x,na.rm=T))}
#testing function sc as identity to check creation of X and Y arrays
#sc <- function(x) x

#save the original mean, sd, min and max of the data_train features for later user
tempmat <- cbind(data.mat$days.numeric,data.mat$daysq, data.mat$daycube, data.mat$first,data.mat$second, data.mat$previous.day.count, counts=time.series.day.counts[-c(1:3)])
minmat <- apply(tempmat, 2, min,na.rm=T)
maxmat <- apply(tempmat, 2, max,na.rm=T)
meanmat <- apply(tempmat, 2, mean,na.rm=T)
sdmat <- apply(tempmat, 2, function(i) sqrt(var(i,na.rm=T)))

#rnn programs usually require 3 dimensional arrays for X and Y
X <- array(c(sc(data.mat$days.numeric),sc(data.mat$daysq), sc(data.mat$daycube), sc(data.mat$first),sc(data.mat$second), sc(data.mat$previous.day.count)), dim=c(1,nrow(data.mat),6))
Y <- array(sc(time.series.day.counts[-c(1:3)]), dim=c(1,nrow(data.mat),1))

#I tinkered with different settings for learningrate, momentum
#batchsize should not be greater than the size of the training set
model <- trainr(Y = Y,
                X = X,
                learningrate = .1, momentum=.1, batchsize=4,
                hidden_dim = c(3), use_bias=T,
                numepochs = 250, network_type="rnn")

# Predicted values
Yp <- predictr(model, X)

# Plot predicted vs actual. Training set + testing set
dv.Y <- (maxmat[7]-minmat[7])*Yp + minmat[7]
dv.Y

#when sq and cubing, center using day mean from training set [1], save.last.row.datamat use [2] or [3] for first and second,respectively
Xnew <- array(c((data_train[nrow(data_train),1]-minmat[1])/(maxmat[1]-minmat[1]), (data_train[nrow(data_train),1]^2-minmat[2])/(maxmat[2]-minmat[2]), (data_train[nrow(data_train),1]^3-minmat[3])/(maxmat[3]-minmat[3]), unlist((save.last.row.data.mat[2]-minmat[4])/(maxmat[4]-minmat[4])),unlist((save.last.row.data.mat[3]-minmat[5])/(maxmat[5]-minmat[5])), 
(time.series.day.counts[length(time.series.day.counts)]-minmat[6])/(maxmat[6]-minmat[6])), dim=c(1,1,6))

X <- array(c(sc(data.mat$days.numeric),sc(data.mat$daysq), sc(data.mat$daycube), sc(data.mat$first),sc(data.mat$second), sc(data.mat$previous.day.count)), dim=c(1,nrow(data.mat),6))

Y <- array(sc(time.series.day.counts[-c(1:3)]), dim=c(1,nrow(data.mat),1))

Yp.new <- predictr(model, Xnew)
Yp.new <- (maxmat[7]-minmat[7])*Yp.new + minmat[7]
Yp.new

plot(data$days[-c(1:3, length(data$days))], time.series.day.counts[-c(1:3)] , col = 'red', type = 'p', main = "RNN for New York State Daily Confirmed Cases\n actual counts (red & solid blue), training (green);  predicted (solid blue)", ylab = "Daily Counts", xlab="Day", xlim=c(0,length(data$days)), ylim=c(0,max(c(Yp.new, diff(data$counts))+10)), cex.main=.7)
points(data$days[length(data$days)], Yp.new,col = 'blue' ,pch=19)
points(data$days[length(data$days)], diff(data$counts)[nrow(data)-1],col = 'red' , pch=19)
lines(data$days[-c(1:3)], c(dv.Y,Yp.new),col="green")
legend(c(0,22),c(max(diff(data$counts)),max(diff(data$counts))-1500) ,legend=paste("Last Day: \n difference between predicted  (blue) and actual  (red):",round(Yp.new-diff(data$counts)[nrow(data)-1],0)),cex=.5)
abline(v=nrow(data)-.5)
text(15,0,"training")
text(nrow(data)+.5,0,"test")

#plot commented out; shows error decrease over iterations
#plot(colMeans(model$error), type="l")
```

I'll be the first to acknowledge that this example is just for demonstration purposes. Our data set is way, way too small to have any chance of estimating meaningful machine learning models, I created a kludgy set of predictors (aka features) without much thought, I haven't included more predictors to the model and I've only focused on number of positive tests (e.g., a better demonstration of machine learning methods would be to use postive testing rates as predictors of outcomes such as subsequent death rates).

There are more diagnostics to consider such as error in prediction and we can iterate over the tuning parameters to find optimal values that give good predictive validity without overfitting.  Note that between day 25 and 40 the overall "trend" of the open red circles is relatively flat (as an average) but the daily variability is increasing.  The current set of predictors I'm  using in this RNN doesn't have the ability to easily capture changes in the variability.  This is one clue for additional predictors (features, see next subsection) that could be added to the model.  This could be an interesting part of the story and something that can be addressed not only with machine learning models but also for the other models we considered in this chapter.  Models that are not capturing this change in variance are possibly missing something important about the underlying process (which could just be something as simple as not all juristictions in New York state are reporting daily but nonetheless an important property to understand if we want to use these approaches to make forecasts).  We could check whether this increasing variance occurs in other states and at different levels of analysis such as the county level. We could check whether this increase in variance is consistent with a multiplicative error model where error (the "plus $\epsilon$" term we discussed earlier) could provide a clue to the underlying process. The point is that there is a potential signal here that our current models are not capturing, and we need to figure out whether we need to move to a different class of models or whether this heterogeneity is due to a simple factor that can be easily addressed. 

Note that this increasing variance was not easy to detect in the cumulative plots and models on cumulative data such as the log linear, exponential and logistic models we covered earlier in this chapter.  If you go back and look at those plots, even the ones using the curve fitting spline methods, they show systematic "under and over the curve" errors consistent with what we are seeing in the incidence plot.  The increasing variance is there but just difficult to spot in the way we examined these data previously. Sometimes converting data to different forms, such as cumulative and incidence forms, helps highlight different aspects of the same data.  It is not easy to see in the barplot of the New York incidence presented at the beginning of the RNN section. It is often useful to cross-check one's interpretation with different model forms and plots.

There are many other approaches that fall under the large machine learning umbrella, these include extensions of RNNs such as long short-term memory deep learning models that can learn patterns further back in time than simple RNNs, general additive models with penalized smoothers, genetic algorithms,  and symbolic regression as well as several packages in R to compute these methods. Overall, I was underwhelmed by the effort so far.  Some of the R packages gave error messages even running the very examples supplied in the package or they made strange predictions.
Part of the issue is that we don't have enough data to  allow these machine learning methods to shine. 

### Feature engineering

To implement machine learning methods for these data I need to spend more time thinking about the feature engineering side of things.  In machine learning parlance feature engineering refers to how the input variables are processed. When one works with time, for instance, it is common to think about multiple ways that time could enter into the model and include all of them. Machine learning algorithms are designed to drop out the irrelevant formulations.  There is a commonly used time series package used in machine learning ([TSFRESH](https://github.com/blue-yonder/tsfresh) in python and an effort to port this to R [tsfeaturex](https://github.com/nelsonroque/tsfeaturex)) that computes several hundred possible measures on each time series (mean, max, min, variance, entropy, auto-correlation, etc) and stores these computations into a large matrix so that they can be used down stream in subsequent analyses.  I didn't put in the effort to go through the sensible features to compute and include those as input into the models so I shouldn't be surprised at the relatively low performance of these models given that I didn't play by the rules of machine learning.

The concept of feature engineering is much like entering linear, quadratic, cubic terms of a predictor into a regression equation.  The idea is that variants of the predictor in the form of a polynomial are treated as predictors. Take that idea, inject it with steroids, and you have feature engineering in machine learning.

As a simple illustration of feature extraction,  here is the output of the daily count variable for each of the 50 states.

```{r warning=F, message=F, echo=longform, results="hide"}
#since this is a package I'm just using here for illustration I'll read it from github and load it here rather than in preliminary.Rmd;
#if you want to download just uncomment the next line
#devtools::install_github("nelsonroque/tsfeaturex")
library(tsfeaturex)

#create just a submatrix of the allstates.long data frame
tsdat <- allstates.long[,c("day.numeric", "state.abb", "count")]
out.featurex <- extract_features(tsdat, group_var="state.abb", 
              value_var="count", features="all", verbose=F, return_timing=F)

#list of features computed by tsfeaturex for each state
names(out.featurex)
```

```{r echo=longform}
#example for just the median
head(out.featurex$f.median)

#double check we get the median if we had computed it by hand
#median(allstates.long$count[allstates.long$state.abb == 
#                                        c("Alaska")], na.rm=T)

```

As you can see tsfeaturex computes 82 different measures on the count data for each state. Those 82 measures could then be included as state-level predictors in a machine learning algorithm using a type of regularizer that introduces sparseness (drops predictors that are not useful in the prediction).

A more appropriate machine learning strategy would be to take the time trajectory data for every country reported in the Johns Hopkins site (about 140 countries) and see if we can learn  patterns about the nature of the trajectories, given various aspects of the country's demographics, dates they instituted various policies (shelter at home), information about their health care system, etc.  One could use the principles developed here about web scraping and gathering these other sources of data, merging them into a single file, and running appropriate machine learning algorithms to find patterns.


An interesting use of machine learning methods (specifically computer vision) is the development of a physical distancing index by a [group at UM](https://voxel51.com/ourstory/).  Physical movement of humans and social distancing is detected in major cities. This [website](https://pdi.voxel51.com/) has some live cameras and charts to illustrate their index.

```{r echo=F}
knitr::include_url("https://pdi.voxel51.com/", height="600px")
```

I'll continue adding to this section and improving the quality of the machine learning example as time frees up from completing other sections.


## Summary

We covered a lot of material in this chapter. We started with a simple linear mixed model on log transformed data that seemingly did a reasonable job of accounting for the patterns across states. The finding that the linear relation in the log scale holds fairly well suggests that there is exponential growth. The ability to make a statement about the underlying process, in this case exponential growth, is far more valuable than finding a good fit to a data set.  But there was a suggestion that the exponential may not be the ideal fit as there was some systematic deviation from a growth curve. The rest of these notes covered a more direct way of fitting exponential growth models through nonlinear regression, a different type of growth model based on the logistic curve and a toy example using a standard machine learning example. 

A key idea I followed in this chapter is not to have a single pattern that represents all states but to use models that simultaneously tell us something about the overall pattern across states (the fixed effect portion of the output) and the variability, or heterogeneity, across the states (the random effect portion of the model).  We could get fancier with this idea where, for example, we could weight the results based on population size of each state, so counts from more populous states receive more weight.  There are  several generalizations of these notions from different perspectives such as statistics and machine learning that you can follow in papers (e.g., varying coefficient models such as one by [Hastie and Tibshirani](https://www.jstor.org/stable/2345993?seq=1) and extensions of tree-based methods like CART in the [R package vcrpart](https://cran.r-project.org/web/packages/vcrpart/index.html)) as well as books ([Hastie et al](https://web.stanford.edu/~hastie/Papers/ESLII.pdf);  [Ramsey & Silverman](https://www.amazon.com/Functional-Data-Analysis-Springer-Statistics/dp/038740080X/ref=sr_1_2?crid=11G4FU2BDD4JU&dchild=1&keywords=functional+data+analysis&qid=1586107101&sprefix=fnctional+data+analysi%2Caps%2C158&sr=8-2)).  Once we properly characterize the shape of the trajectories and the variability across units, we can then study the associations between predictors and the parameters of the model, such as I attempted to do with the state-level poverty data and a parameter from the logistic growth model.
