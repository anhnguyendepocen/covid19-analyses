# Reading Data into R {#intro}


This chapter will cover three different ways to read data into R: 1) executing a pull from a git repository, 2) scraping data from the web, and 3) pulling data with an api from the national census.

## Comments about R code

I am writing these notes to serve as a tutorial for how to work with data. My goal is not to teach data science or statistics in these notes but rather to illustrate how to work with data and highlight some pros and cons of different approaches. I will emphasize the code so that interested readers can see what I did. You are free to take this code and use it for your own purposes.

My style of writing code is idiosyncratic as I suspect it is for everyone who writes code.  R is a complex language and like most languages there are various dialects.  I have learned many of those dialects and use them frequntly, sometimes mixing and matching the dialects to create hybrids.  I'll try to maintain some consistency but can't promise I'll succeed.  Desiderata include

1.  use ggplot2 as much as possible for plots rather than the original base R plotting (though sometimes I can't resist)

2.  write code that is robust to possible changes in the data that are automatically read in, and

3.  write code that is easy to read and follow even though it may not be efficient code.  

Sometimes these present conflicts such as writing robust code may not be code that is easy to follow.


## Pulling from a git repository

I'll use the data base that Johns Hopkins has been maintaining, which is also used by news outlets such as CNN.  I already cloned the git repository on my local computer so each day I merely have to issue a "git pull" command to get all new files that have been edited.  This is a better process than conducting a completely new download every day of the entire repository.

If you want to learn more about working with git repositories, you can do a google search for how to clone a repository.  Rstudio also has features to help clone and manage a git repository. 

This script will switch to the folder I'm keeping the git repository, execute the command "git pull" and then bring me back to the original folder.  To keep things organized I'm keeping the git repository in a seperate folder from the R files that are creating these pages.



```{r}
system("pushd '/Users/gonzo/Dropbox/transfer/mac transfer/COVID-19' && git pull && popd")
```

Now read the file that has the cumulative counts of confirmed covid-19 cases by country and, if relevant, state or province. I'll save it into an object called datacov; I list the first 6 rows of datacov.

```{r}
datacov <- read_csv("/Users/gonzo/Dropbox/transfer/mac transfer/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Confirmed.csv")
head(datacov)
```

The column labels the object datacov are by date starting in column 5. The column names are special string variables because they start with numbers and R doesn't like variable names to start with numbers. For example, \`1/22/20\` (backticks), and you'll see in later syntax where I need to refer to these columns using the backwards apostrophes.  THe other columns contain province/state and counry/region labels as well as latitude and longitude.

The column names though get converted to regular strings when treated as names.  You can also check this to see if it includes yesterday's date as the last entry. The data seems to be updated late in the evening so if these commands are run late in the evening you may see today's date as the most recent date.

```{r}
colnames(datacov)
```

A few words about these this data set.  All we have are the counts of confirmed cases. This data set does not include other important information like the number of tests conducted or the population size for each unit.  It makes it difficult to evaluate the total number of confirmed cases without knowing how many tests that unit conducted or its population size. When evaluating changes over time in counts of confirmed cases, for example, we don't know if the total counts are increasing because more people are getting sick or the unit is also increasing the number of tests they conducted. Further, it becomes difficult to compare counts of confirmed covid-19 cases across units without knowing whether the two units have different testing rates, different sample sizes, different population densities, etc.  Basically, counts of confirmed cases are quite limited in the information they provide.  Below I'll scrape testing data by US state as well as population totals by US state, and we'll use these numbers in subsequent analyses.

## Scraping data from the web example

This is a method using commands in the rvest library to read an html page and extract the table as well as stringr to do some string manipulations. The user needs to supply the URL.

There is always additional manipulation needed to format the table, here I had to rename columns, remove characters from the country names dealing with footnotes that appeared in the original page, and other little issues documented below.

If the table on the wiki changes, it could very well break this code.  This already changed in that originally the table I want was the 4th table and now it is the 5th table.

```{r}
URL <- "https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic"

wikipage <- read_html(URL)

#the 5th table in this html page is the country totals (so .5 below)
#the number 5 is hardcoded here, not great programming style
table <- wikipage %>%
html_nodes("table") %>% .[5] %>%
html_table(fill=TRUE, header=T)

#delete 1st row (incorrectly read in the header) and last three rows which are notes
table <- table[[1]]
table <- table[-c(1,(nrow(table)-1):nrow(table)),-c(1,6)]
colnames(table) <- c("location","cases","deaths","recov")

#drop [*] footnotes from country names in col 1
#could use fancier regex or perl style script
#if last char is "]" then delete the last 3 characters
#k is a temporary placeholder to make the code more readable as k appears multiple times in the 2nd line
k <- table[,1]
k <- ifelse(substr(k,nchar(k),nchar(k)) == "]", substr(k,1,nchar(k)-3), k)
table[,1] <- k

#change label to sum referring to sum of all countries
table[1,1] <- "World Sum"

#print first 10 rows
head(table, 10)

#save into object
wiki.corona.counts <- table
```

Now this table is ready to use in analyses.  It differs from the datacov Johns Hopkins data.frame I downloaded in the previous section because in addition to counts of confirmed cases it has number of deaths as well as number recovered.

## Using APIs

For later analyses I'll need to know current population sizes. Here I'll just focus on US states.  The files I downloaded are covid-19 counts by state and country. They do not have population size information, which is important information to evalate the total number of covid-19 cases in a unit. 

This code makes use of the library tidycensus (and also functions in tidyverse). To access the national census data base one needs to register with the census site and receive a unique key.  My unique key is not printed here to keep it private. If you want to run this code yourself, you would need to register with the national census and then enter the key you receive as an argument to the census_api_key() command below. See the documentation of the tidycensus package for more information; here is the registration page to get your own key [registration](https://api.census.gov/data/key_signup.html)

```{r eval=F, echo=FALSE}
census_api_key(key="9fdfc1d46398c3a9e30971df2850174583db8a48", install=T)
```

```{r}
#uncomment this line and enter your key inside the quotes
#census_api_key(key="YOUR.KEY.GOES.HERE", install=T)

states <- unique(fips_codes$state_name)

#drop DC and territories
states <- states[-c(9,52:57)]
states.abb <- unique(fips_codes$state)[-c(9,52:57)]

#go to the census, get population data (product) by state (geophaphy) using states listed in the object states
state.population <- get_estimates(geography="state", state=states, product="population")
state.population <- subset(state.population, variable == "POP")

#create state abbreviations with popoulation size in million like "39.6 CA" for California 39.6 million
#this is for shorter, more informative labels in plots I'll do later
state.population$prettyval <- paste(format(round(state.population$value / 1e6, 1), trim = TRUE), states.abb)
state.population$prettyval <- fct_reorder(state.population$prettyval, state.population$value)
```

Here is the first 10 rows of the state.population data.frame so you can see the result of these commands. 
```{r}
head(state.population,10)
```

## Number tested in US

Another important piece of information we need are the number of tests that have been conducted. Is the total count in a region high or low because the infection rate is high or because more or fewer tests (respectively) have been conducted?

As of 3/21/20 this information is spotty.  We can go to the CDC website to collect this information.  The website now lists testing data for both CDC and Public Health Labs.  After scraping the data off the CDC website I had to do some cleaning, parsing data into numbers as some cells had extra characters that are the footnote symbols appearing on the website, and had to change the format of the date column to be compatible with the date format I'm using elsehwere in these notes.


```{r warning=F, message=F}
URL <- "https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/testing-in-us.html"

wikipage <- read_html(URL)

table <- wikipage %>%
html_nodes("table") %>%
html_table(fill=TRUE, header=T)
table <- table[[1]]


#drop extra character from 2nd and 3rd columns
#k is a temporary placeholder to make the code more readable as k appears multiple times in the 2nd line
k <- table[,2]
#nice function readr package to extract the numerical part of a string
table[,2] <- parse_number(k)

k <- table[,3]
#nice function readr package to extract the numerical part of a string
table[,3] <- parse_number(k)


#fix date
k <- table[,1]
table[,1] <- as.Date(paste0(k,"/20"), "%m/%d/%y")

cdc.testing <- table

#last 6 rows of cdc.testing to see what we created 
tail(cdc.testing)

#total tested in CDC and Public Health
apply(cdc.testing[,c(2,3)],2,sum,na.rm=T)

#day totals: testing totals regardless of CDC or Public Health Lab
cdc.testing$total.tests <- apply(cdc.testing[,c(2,3)],1,sum,na.rm=T)
```

There is another site I found that reports positive and negative results by state. This is the [COVID tracking project](https://covidtracking.com).  I don't know how reputable this source is but I'll keep an eye on it. 
It reports daily records by state on number of positive,  negative, pending, hospitalized, death, and total number of tests.  This page points out specifics about each state (e.g., Maryland stopped reporting negative cases as of 3/12, MA numbers include repeated testing on the same individual, not all states include data from commercial labs) showing that it is challenging to interpret such data at the aggregate level when there is so much heterogeneity across units. See, for example, [state-level details](https://covidtracking.com/data/)


```{r warning=F, message=F}
covid.tracking <- read_csv("https://covidtracking.com/api/states/daily.csv")
head(covid.tracking)

#focus just on the 50 states (i.e., drop DC and territories like American Samoa AS)
covid.tracking <- subset(covid.tracking, covid.tracking$state %in% state.abb)
```

## Conclusion

Just because data files can be created does not mean those data are meaningful.   There are differences across these  data sources and some data are difficult to compare across states or over time (e.g., reporting of testing differs across states, different types of tests are used across states, the type of testing may have changed during the time interval). The myth is that if you have a large enough data set these kinds of issues will wash out in the aggregrate.  Maybe random error will wash out, but large data sets do not necessarily eliminate, by virtue of their size alone, systematic differences or biases.  The size of the data set can impress and ease one into passive acceptance of a particular data pattern and interpretation. We must exercise the same skepticism we privledge small data sets, and be aware that large data sets may have additional issues not present in small-scale studies. 

A lot of code was presented in this chapter, but the code is in service of downloading the relevant up-to-date data I wanted to automate everything and write code for all my data manupulations so that you could see what I did and to make the output completely reproducible. One benefit of this approach is that I can run this code every day to get the current information so that all my analyses and plots in this website automatically update. Any workflow process that would require that I manually download a spreadsheet and manually edit that file would not be easily reproducible and would create extra work for me each day when I update this website.

::: {.infobox .caution data-latex="{caution}"}

**R Notes**

In some workflows  I save my workspace at this stage when all the data have been read in and processed, so that subsequent files can just load the workspace and pick up from here. This saves time because the same operations don't have to be redone.  But because these covid-19 data downloads happen often it is better to recreate the workspace to avoid issues with newer data being available.  The R functions save.image() and load() usually work well when data remain static, such as after data collection is completed and no further data cleaning is needed.

:::


