# Descriptive Plots {#descplot}

This chapter  presents some descriptive plots of the raw data. No analyses, no standard errors, no parameters to estimate, no hypotheses to test. Just raw data presented in different forms to gain insight into the patterns that are in the data.  I'll create a few plots at the world level, and then switch to the state level for US data.  You are free to take my code and edit it to produce additional plots for other countries and their states or provinces (up to the degree of resolution provided by the data sets we downloaded, which as of 3/23/20 is at the US county level).  Throughout these notes I'll focus on the counts and rates of testing positive. I'll defer death counts and rates until the chapter on process models. 

## World Map


Here is an example I copied from [here](https://datascienceplus.com/map-visualization-of-covid19-across-world/)

I make use of the datacov file I read in from the Johns Hopkins git repository.



```{r warning=F, message=F}
#for these code chunks I use the datacov.World data file
#coding trick; by switching the definition of datacov I can keep my original code post the 3/23/20 change and just swap 
#out which file, world or us, to drop into datacov.  Rest of code should remain the same as prior to the change.
datacov <- datacov.World

# cutoffs and labels to use in the plot based on the number of cases
mybreaks <- c(1, 20, 100, 1000, 50000)
mylabels <- c("1-19", "20-99", "100-999", "1,000-49,999", "50,000+")

world <- map_data("world")

ggplot() +
 geom_polygon(data = world, aes(x=long, y = lat, group = group), fill="grey", alpha=0.3) +
 geom_point(data=datacov, aes(x=Long, y=Lat, size=datacov[[ncol(datacov)]], color=datacov[[ncol(datacov)]]),stroke=F, alpha=0.7) +
 scale_size_continuous(name="Cases", trans="log", range=c(1,7),  breaks=mybreaks, labels = mylabels) +
    scale_color_viridis_c(option="inferno",name="Cases", trans="log",breaks=mybreaks, labels = mylabels) +
    theme_void() + 
    guides( colour = guide_legend()) + 
  labs(caption = 
  "Data Repository by Johns Hopkins CSSE. Visualization by DataScience+ ") +
    theme(
      legend.position = "bottom",
      text = element_text(color = "#22211d"),
      plot.background = element_rect(fill = "#ffffff", color = NA), 
      panel.background = element_rect(fill = "#ffffff", color = NA), 
      legend.background = element_rect(fill = "#ffffff", color = NA)
    )

```

Even more sophisticated is the  [interactive map](https://www.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6) developed at Johns Hopkins (same group that provides the key data we downloaded in Chapter \@ref(intro)), where you can zoom in and out, click on different countries (left panel) and see data for that country on the right panel, etc.

### World Map with Time Animation

I'll expand the example I just did in R to make an animation so we can see the total cases in the world map change by day.  We need to reformat the data file datacov into long format in order to be used by the animation function in the package gganimate.  That is, rather than using the datacov data.frame that is in wide format, I need to reformat the data.frame into long format and create a day column and a count column.  The gather() function in dplyr does this very efficiently.  To help in later analyses I also create a day.numeric variable that is 0 on 3/08/20 and increments by one each day after.  I'll explain later why I set 3/08/20 as day 0.

```{r}
datacov.long <- gather(datacov, day, count, `1/22/20`:(names(datacov)[ncol(datacov)]))
datacov.long$day <- as.Date(datacov.long$day, "%m/%d/%y")
datacov.long$day.numeric <- as.numeric(datacov.long$day)-18283
```

Now the animated plot by day. I rewrote the original parts of the plotting program to make it more efficient for the animation (e.g., I use the ggthemes in the maps package). This animation uses the package gganimate to reproduce the plot for each day and then overlays the plots one on top of the other to produce the animation.

```{r cache=cache, warning=FALSE, message=FALSE,fig.cap="World Count of Positive Cases"}
ggplot(datacov.long) +  borders("world", colour = "gray90", fill = "gray85") +
  theme_map() + 
  geom_point(aes(x=Long, y=Lat, size=count, color=count),stroke=F, alpha=0.7) +
 scale_size_continuous(name="Cases", trans="log", range=c(1,7),  breaks=mybreaks, labels = mylabels) +
    scale_color_viridis_c(option="inferno",name="Cases", trans="log",breaks=mybreaks, labels = mylabels) +
    theme_void() + 
    guides( colour = guide_legend()) +
    theme(
      legend.position = "bottom",
      text = element_text(color = "#22211d"),
      plot.background = element_rect(fill = "#ffffff", color = NA), 
      panel.background = element_rect(fill = "#ffffff", color = NA), 
      legend.background = element_rect(fill = "#ffffff", color = NA)) + 
  labs(title="Date: {frame_time}", caption = "Data Repository provided by Johns Hopkins CSSE") +
  transition_time(day) + ease_aes('linear')
```


While counts are useful (see [Preface](#preface) for a discussion of London's cholera epidemic) they have their limitations. Does a country have a relatively high number of positive cases because it has a large population?  In this field it is common to normalize counts by the population and report numbers per capita (e.g., 62 cases out of 100,000).  I'll have to track down the populations of each country and merge that information into this animation.  Later in this chapter, I develop this animation for the US and I do have the state populations so will illustrate the difference in this animation in counts versus per capita there.  However, raw counts still have a role in helping to inform the impact of covid-19.  If a city has 1500 available hospital beds (and presumably the staff and supplies to provide care for  those  beds), but there are 2000 people in need of hospitalization, then there is a public health issue and the 'per capita" concern becomes moot.


## Compare World Counts to US

It would be helpful to see the pattern over time of the world counts of confirmed covid-19 cases relative to the US counts.  Here I create a subset of the datacov data.frame that includes just the 50 states (I dropped DC and the US territories for this analysis).

```{r}
#keep this to make this code chunk work (i.e. temporarily revert to the datacov.ori file)
#revert back to old datacov
datacov <- datacov.ori

#new code with datacov.US 3/31/20 edits
datacov.temp <- datacov.US
allstates.temp <- subset(datacov.temp, `Province/State`%in% states)
allstates.temp2 <- allstates.temp %>% group_by(`Province/State`) %>%  summarize_at(vars(`1/22/20`:(names(allstates.temp)[ncol(allstates.temp)])), sum,na.rm=T)
#need state level long and lat (these are county) so just replace these sums with the ones in the old allstates

#create subset of 50 states from original datacov in order to have the correct long and lat by state (rather than county)
allstates <- subset(datacov.ori, `Province/State`%in% states)
allstates.ori <- allstates


allstates.temp2 <- allstates.temp2[order(match(allstates.temp2$`Province/State`,allstates$`Province/State`)),]
#double check ordering of rows
head(cbind(allstates$`Province/State`, allstates.temp2$`Province/State`))

allstates.temp2$Lat <- allstates$Lat
allstates.temp2$Long <- allstates$Long
allstates.temp2$`Country/Region` <- allstates.ori$`Country/Region`

#reorder columns to be like allstates.ori so subsequent code works
allstates <- allstates.temp2[,c(1,ncol(allstates.temp2), ncol(allstates.temp2)-2, ncol(allstates.temp2)-1, 2:(ncol(allstates.temp2)-3))]

#now everything is set so new data structure looks like the old data structure

#prettyval is an extra column appended at the end; it has population size and state abbreviation like "19.5 NY"
#need to be sure the row orders of state.population that we downloaded from the census website
#match the order of states in the allstates subset of the datacov data.frame
allstates$prettyval <- state.population$prettyval[order(match(state.population$NAME,allstates$`Province/State`))]

#likewise, save state populations from the census file to the allstates data.frame
allstates$population <- state.population$value[order(match(state.population$NAME,allstates$`Province/State`))]
     
#number of variables added (prettyval and population) to the allstates data.frame
cadded <- 2
#counts by dates are in columns starting with `1/22/20` and ending in ncol(allstates)-2 because
#we appended two columns at the end
allstates.long <- gather(allstates, day, count, 
                         `1/22/20`:(names(allstates)[ncol(allstates)-cadded]))

#reformat the day column to be Dates that R can understand
allstates.long$day <- as.Date(allstates.long$day, "%m/%d/%y")

#in some analyses it may be helpful to look at first order differences in counts (day (t+1) - day (t))
#so far I don't need this but I leave it here
#allstates.long$count.diff <- c(0,diff(allstates.long$count))
```

More data manipulation needed.  Turns out that the Johns Hopkins data did not report state-level data prior to 3/08/2020 (just US totals apparently).  So for state-level analyses it may be helpful I only keep dates after 03/08/2020.  Also, in the comments I explain how R handles dates with a reference date of 01/01/1970. 

```{r}
allstates.long <- subset(allstates.long, day > "2020-03-08")

#in case we need the last dates for each state
# data_set_ends <- allstates.long %>% group_by(`Province/State`) %>% top_n(1,day) %>% pull(prettyval)

#create day.numeric starting with 3/09/20 as day 1.  in R, Dates have 01/01/1970 and 03/08/20 is 18330 days
#from the origin so need to subtract 18330 from all the days. Essentially, I'm centering
#at 03/08/20
allstates.long$day.numeric <- as.numeric(allstates.long$day)-18330

#make the variable with the state abbreviations a factor in R
allstates.long$state.abb <- factor(allstates.long$`Province/State`)

#create per capita (out of 100)
allstates.long$percap100 <- allstates.long$count/allstates$population * 100
```

Now that we have allstates.long created we can compare cumulative counts in the World to the US cumulative counts.  For a definition of the multiplot() function see the [Preliminaries chapter](#multiplot).

```{r,fig.cap="Cumulative Counts of Positive Cases" }
#compute sums for the entire world by day and store plot in object p1
sum.world <- data.frame(count=apply(datacov.World[,5:ncol(datacov.World)],2,sum,na.rm=T))
sum.world$day <- as.Date(rownames(sum.world), "%m/%d/%y")
p1 <- ggplot(sum.world, aes(x=day,y=count))  + geom_bar(stat="identity",fill="steel blue") + ggtitle("World Wide Counts") + theme_minimal() + scale_y_continuous(labels=comma)
#above line: labels=comma prints y axis labels as numbers rather than scientific notation
#can print plot p1 by itself by uncommenting next line
#p1

#using weight option rather than y so that bar stat_count can do the sum (saves having to compute sum and then do what I did for sum.world)
comparesums <- allstates.long %>% group_by(day) %>% summarize_at("count", sum,na.rm=T)
comparesums
#seems coding was by US then by state at about 3/08
#here is a command to produce total US counts for all days in the data set starting with 1/22/20 but it has lots of 0s
#data.frame(count=apply(allstates[,5:(ncol(allstates)-cadded)],2,sum,na.rm=T))

#I want to draw a horizontal line in the cumulative plot that corresponds to the 
#US fraction of the world population
uspopfrac <- 330/7800
cv10frac <- uspopfrac*sum.world[nrow(sum.world),1]

#had to add +1 to max date in scale_x_date
p2 <- ggplot(allstates.long, aes(x=day,weight=count))  + geom_bar(na.rm=T,fill="steel blue") + ggtitle("US Counts") + scale_x_date(limits=c(sum.world$day[1], sum.world$day[nrow(sum.world)]+1))  + theme_minimal() + geom_hline(yintercept=cv10frac,col="red") + scale_y_continuous(labels=comma)
#+ scale_y_continuous(limits=c(0,max(sum.world$count)))
#can print plot p2 by itself by uncommenting next line
#p2

#produce a single plot that has the world cumulative count at top and the US cumulative count at the botton, along with a
#red line indicating the count that corresponds to the fraction of the world population that represents the US
#if US counts are above the red line it means the counts of covid=19 are greater than what would be expected by a random 
#process of covid-19 just appearing randomly based merely on population size alone

#the plot multiplot is defined in chapter preliminaries
multiplot(p1,p2,cols=1)
```

## US State-Level Plots

I decided to plot percentage relative to population (i.e., counts/population * 100).  The numbers are small. I've seen people report this as cases per 100,000, but I decided to stick with cases per 100 to maintain the percentage interpretation.  This is just a scale issue and doesn't affect the plots or the analyses.   The labels in the plots, like "7.5 WA" means Washington state with a population of 7.5 million. These labels are the prettyval labels I created earlier.

```{r fig.cap="Per Cap Positive Cases by State"}
#most recent date in file
max.day.numeric <- max(allstates.long$day.numeric)
max.day <- max(allstates.long$day)

#extend plot region on the right to allow room for state labels
extend.days <- 4

allstates.long%>%mutate(label = if_else(day == max(day), as.character(prettyval), NA_character_)) %>%
ggplot( aes(x=day, y=count/population*100,group=prettyval, color=prettyval)) + geom_line() +
 geom_label_repel(aes(label= label), nudge_x=2, na.rm=T,segment.color = 'grey50', label.size=.01, size=2.5, show.legend=F) +
coord_cartesian(clip = 'off') + scale_x_date(limits=as.Date(c("2020-03-09", as.character(max.day+extend.days)))) + 
    theme(legend.position="none", plot.margin = margin(0.1, 1, 0.1, 0.1, "cm"))
```

Same plot but now putting the vertical axis on the log (base 10) scale.  An exponential process becomes linear when taking logs so we would expect to see a pattern that closely resembles straight lines if the count of confirmed cases follows an exponential form.  Each state can have a different growth rate, which will show up as different slopes.  A straight line representation makes sense though there is some heterogeneity across states in the intercept and maybe some slight differences in the slope, which translates into the growth rate. West Virginia (WV) didn't show its first case until 3/17/20 so that is why the green curve (lowest) looks different than the rest.

```{r warning=F, message=F, fig.cap="Per Cap Positive Cases by State (log10 scale)"}
subset(allstates.long, day.numeric!=0) %>%mutate(label = if_else(day == max(day), as.character(prettyval), NA_character_)) %>%
ggplot( aes(x=day, y=count/population*100,group=prettyval, color=prettyval)) + geom_line() +
 geom_label_repel(aes(label= label), nudge_x=2, na.rm=T,segment.color = 'grey50', label.size=.01, size=2.5, show.legend=F) +
coord_cartesian(clip = 'off') + scale_x_date(limits=as.Date(c("2020-03-09", as.character(max.day+extend.days)))) + 
    theme(legend.position="none", plot.margin = margin(0.1, 1, 0.1, 0.1, "cm")) + scale_y_continuous(trans='log10')

```

To help see the state by state structure in these curves (e.g., whether or not they are linear in the log scale), I partitioned the 50 states by population (sample sizes in each panel are 12 or 13 states). The states with larger populations (two lower panels) show relatively linear patterns.  The smaller in population states (two upper panels) show some curvature but that could be due to several states not showing cases until a few days later.  

```{r warning=F, message=F, fig.cap="Per Cap Positive Cases by State (log10 scale); Panels are based on Population"}
subset(allstates.long, day.numeric!=0) %>%mutate(label = if_else(day == max(day), as.character(prettyval), NA_character_)) %>%
ggplot( aes(x=day, y=count/population*100,group=prettyval, color=prettyval)) + geom_line() +
 geom_label_repel(aes(label= label), nudge_x=2, na.rm=T,segment.color = 'grey50', label.size=.01, size=2.5, show.legend=F) +
coord_cartesian(clip = 'off') + scale_x_date(limits=as.Date(c("2020-03-09", as.character(max.day+extend.days)))) + 
    theme(legend.position="none", plot.margin = margin(0.1, 1, 0.1, 0.1, "cm")) + scale_y_continuous(trans='log10') + facet_wrap(~cut2(population, g=4,m=8))

```

Here is another version of the plot where I set 0 to missing value (NA) so that the curves begin where there are nonzero points. The only difference is the 2nd line with the mutate() and na_if() command. This modification to the plot makes it easier to detect linearity as we don't have the artificial jump in the curve from 0. But, while it makes the plots a little easier to understand, it drops the important information about _liftoff_, the point at which the count moves from zero to nonzero.  This is something that could be modeled with a parameter in the structural model and so, in principle, one could see what factors affect the liftoff.  Another issue is that log(0) is undefined so you'll see many sites are preparing such graphs will set a minimum number of cases (e.g., the day at which the state reached 10 cases) before they start plotting the curve for that state.

```{r warning=F, message=F, fig.cap="Per Cap Positive Cases by State (log10 scale); Panels are based on Population; Os dropped"}
subset(allstates.long, day.numeric!=0) %>%mutate(label = if_else(day == max(day), as.character(prettyval), NA_character_)) %>%
  mutate(count = na_if(count, "0")) %>%
ggplot( aes(x=day, y=count/population*100,group=prettyval, color=prettyval)) + geom_line() +
 geom_label_repel(aes(label= label), nudge_x=2, na.rm=T,segment.color = 'grey50', label.size=.01, size=2.5, show.legend=F) +
coord_cartesian(clip = 'off') + scale_x_date(limits=as.Date(c("2020-03-09", as.character(max.day+extend.days)))) + 
    theme(legend.position="none", plot.margin = margin(0.1, 1, 0.1, 0.1, "cm")) + scale_y_continuous(trans='log10') + facet_wrap(~cut2(population, g=4,m=8))

```

This pattern is quite remarkable. The states appear to have similar slopes on this log plot. They vary in intercept, but that reflects when the state started reporting positive cases.  It seems states are on a similar growth trajectory, but some states are further along (higher intercepts) than other states.  That the log transformation of the Y axis converted these curves into lines is consistent with the underlying pattern of these data following an exponential model. In the next chapter I'll cover the exponential more directly, both in its log linear form as in these graphs as well as through nonlinear regression.



## Animated Map of US

Here is the animation for the US.  Not much to look at since it has only been about 2 weeks of data.  This map needs work as Alaska and Hawaii outlines are not printed (their data points appear on the left side roughly where the states would be on the map).  I'll need to do some coding to create an inset for Alaska and Hawaii (possible clue to inset: https://stackoverflow.com/questions/49523375/graphing-lat-long-data-points-on-us-map-50-states-including-alaska-hawaii

```{r warning=F, message=F, cache=cache,fig.cap="US Count of Positive Cases"}
mybreaks <- c(1, 20, 100, 500, 1000, 2000, 10000)
mylabels <- c("1-19", "20-99", "100-499", "500-999", "1,000-1,999", "2,000-9,999","10,000+")

ggplot(allstates.long) +  borders("usa", colour = "gray90", fill = "gray85") +
 theme_map() + 
  geom_point(aes(x=Long, y=Lat, size=count, color=count),stroke=F, alpha=0.7) +
 scale_size_continuous(name="Cases", trans="log", range=c(1,7),  breaks=mybreaks, labels = mylabels) +
    scale_color_viridis_c(option="inferno",name="Cases", trans="log",breaks=mybreaks, labels = mylabels) +
    theme_void() + 
    guides( colour = guide_legend()) +
    theme(
      legend.position = "bottom",
      text = element_text(color = "#22211d"),
      plot.background = element_rect(fill = "#ffffff", color = NA), 
      panel.background = element_rect(fill = "#ffffff", color = NA), 
      legend.background = element_rect(fill = "#ffffff", color = NA)) + 
  labs(title="Date: {frame_time}", caption = "Data Repository provided by Johns Hopkins CSSE") +
  transition_time(day) + ease_aes('linear')
```

Because I have the state-level population sizes I can redo the animation using a per capita normalization out of 100.  I modified the breaks and labels manually but there may be a way to do that automatically with ggplot2 features.

```{r warning=F, message=F, cache=cache, fig.cap="US Per Cap (100)"}
temp <- cut2(allstates.long$percap100, g=5, m=5)
mylabels <- levels(temp)
#onlycuts = T saves the numeric break points, drop first element which is 0
mybreaks <- cut2(allstates.long$percap100, g=5, m=5, onlycuts=T)[-1]

ggplot(allstates.long) +  borders("usa", colour = "gray90", fill = "gray85") +
 theme_map() + 
  geom_point(aes(x=Long, y=Lat, size=percap100, color=percap100),stroke=F, alpha=0.7) +
 scale_size_continuous(name="Cases", trans="log", range=c(1,7),  breaks=mybreaks, labels = mylabels) +
    scale_color_viridis_c(option="inferno",name="Cases", trans="log",breaks=mybreaks, labels = mylabels) +
    theme_void() + 
    guides( colour = guide_legend()) +
    theme(
      legend.position = "bottom",
      text = element_text(color = "#22211d"),
      plot.background = element_rect(fill = "#ffffff", color = NA), 
      panel.background = element_rect(fill = "#ffffff", color = NA), 
      legend.background = element_rect(fill = "#ffffff", color = NA)) + 
  labs(title="Date: {frame_time}", caption = "Data Repository provided by Johns Hopkins CSSE") +
  transition_time(day) + ease_aes('linear')
```

## Incidence Plots {#incidenceplots}

It is common with these type of time series data to work with what are called first order differences. Rather than look at cumulative counts you look at day to day differences in the cumulative counts, or equivalently, the number of new cases each day.  This is what give rise to the "curves" when people talk about "flattening the curve". Think of this as daily counts and we merely compute histograms.  This section was adapted from [Tim Chruches Blog](https://timchurches.github.io/blog/posts/2020-02-18-analysing-covid-19-2019-ncov-outbreak-data-with-r-part-1/)

Here is the daily incidence rate using the same US data (starting on 3/10/20 and not including DC and the territories).   The curve does not appear to be flattening yet.

```{r incplot, warning=F, message=F}
allstates.temp <- allstates[,c(1:4,52:(ncol(allstates)-2))]
allstates.temp <- allstates.temp[,-5]

#create new data frame with differences of total counts which equals daily totals
day.sums <- dplyr::select(allstates.temp, `3/10/20`:ncol(allstates.temp)) %>% colSums(na.rm=T)
day.sums <- data.frame(day = as.Date(names(day.sums),"%m/%d/%y"), count = day.sums)
#this is the line that computes differences
day.sums$daily <- c(NA,diff(day.sums$count))

#make plot with nice labels
#5th bar looks almost empty; the data show 61 cases that day 
ggplot( day.sums, aes(x=day,y=daily)) +  geom_bar(stat="identity") +
    scale_x_date(date_breaks="1 week", date_labels = "%d %b") +
    labs(y="Daily incremental incidence",
                                   title="Positive US Covid-19 Cases")  +
    theme(legend.position = "none",
          strip.text.y = element_text(size=11))
```


## Use of PCA to check for outliers

Here is yet another use for PCA.  We can use it to detect outliers in a complicated data set. Let's take the 50 states and the time series from 3/09/20 to present, to create a 50 by day data matrix of positive counts.  Then compute a PCA of the correlation matrix between states. This correlation matrix is 50 x 50 and represents how similar each state's cumulative trajectory is to another state's cumulative trajectory for all possible pairs of states.  Plot the factor scores of the 50 states on the first two PCs.  The majority of the states will cluster together. The outlier states, those with very different trajectories, appear far away from the primary cluster suggesting they have a very different trajectory. The three candidate outliers are the 3 states early in the news:  NY, WA and CA. Worth checking why MA and NJ are slightly away from the cluster. Recall the numbers in front of the state abbreviation correspond to the state population in millions.  This type of PCA is commonly done in biological modeling to check for outliers, sometimes not on the raw data but the set of parameters that are estimated for each unit (e.g., if you run regressions for each state, gather the betas from those regressions as data such as 8 betas per state, then compute a correlation matrix across states, then run a PCA on that correlation matrix to detect outliers). 

```{r fig.cap="PCA to identify states with candidate outliers", warning=F}
temporal.data <- allstates[,53:(ncol(allstates)-2)]
rownames(temporal.data) <- allstates[,"prettyval"]$prettyval

pca.out <- prcomp(temporal.data, center=T, scale=T)

ggplot(data.frame(PC1=pca.out$x[,1], PC2=pca.out$x[,2], labels=rownames(pca.out$x)), aes(x=PC1, y=PC2, label=labels)) + geom_text(size=4)

```
