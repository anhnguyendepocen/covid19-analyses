# Deeper Understanding {#deeperunderstanding}

In this chapter I consider some hypotheses that can be tested with relatively simple methods.

## Regression Discontinuity

This subsection will show a fun test that we can examine in real time.  Some governors of US states have decided to reopen businesses much sooner than recommended by public health officials.  For example, Georgia has decided to reopen gyms, massage therapy, tatoo shops, barbers and hair stylists on April 24th with restaurants, theaters and private social clubs opening up on the 27th.  It will be  difficult to practice social distancing while getting a tatoo or a body piercing.

Pending:  I'll plot the data for Georgia by county 3 weeks before the 24th and 3 weeks after.  It will take a few days, possibly up to 14 days, to see the effects on the positive testing rate due to a higher rate of social contact, and maybe up to two weeks to see any effects on the death rate.  I'll examine a similar plot for an adjacent state that has not relaxed their social distancing mandate for comparison.

This example will illustrate the use of the regression discontinuity (RD) design, though before the causality police get on my case for not clarifying my assumptions I'll point out that I won't be using this to claim I have estimated the causal effect of social distancing.  I'll illustrate the idea of this design, its characteristic plot and  point to papers where the reader can learn about the various approaches to extracting additional information from this type of research design.

The city of Atlanta is in Fulton county, which is the top line.

```{r warning=F, message=F, echo=longform}
datacov.US.long <- gather(datacov.US, day, count, `1/22/20`:(names(datacov.US)[ncol(datacov.US)]))
datacov.US.long$day <- as.Date(datacov.US.long$day, "%m/%d/%y")
datacov.US.long$day.numeric <- as.numeric(datacov.US.long$day)-18283

# drop two counties because unassigned or out of GA; total 159 counties per wiki 
# https://en.wikipedia.org/wiki/List_of_counties_in_Georgia
georgia.df <- subset(datacov.US.long, `Province/State`=="Georgia") %>% filter(!Admin2 %in% c("Out of GA", "Unassigned") )

# all county plot
# ggplot(subset(georgia.df, day > as.Date("2020/04/1")), aes(day,count, group=Admin2, color=Admin2)) + geom_line() + ggtitle("Georgia counties") 

# focus just on counties with cases > 250 as of the compilation of the report
# red vertical line at 4/24 and blue vertical line at 4/27
georgia.df.max <- georgia.df %>% group_by(Admin2) %>% mutate(maxcount = max(count,na.rm=T)) %>% ungroup()
range.ylim <- range(georgia.df.max$maxcount)

# add two regression lines to plot before/after cutoff
georgia.df.max$cutoff <- factor(ifelse(georgia.df.max$day<=as.Date("2020/04/24"),"before","after"))

subset(georgia.df.max, day > as.Date("2020/04/1") & maxcount>=250) %>% 
  mutate(label = if_else(day.numeric == max(day.numeric), as.character(Admin2), NA_character_)) %>%
ggplot(aes(day,count, group=Admin2, color=Admin2)) + geom_line() + ggtitle("Georgia counties with greater than 250 cases")  + geom_vline(xintercept=as.Date("2020/04/24"), color="red") + ylim(range.ylim) + theme_bw() +
   geom_label_repel(aes(label= label), nudge_x=2, na.rm=T,segment.color = 'grey90', label.size=.01, size=2.5, show.legend=F) + scale_x_date(limits=c(as.Date("2020/04/1"), as.Date(max.day+5))) +
    theme(legend.position="none", plot.margin = margin(0.1, 1, 0.1, 0.1, "cm")) #+
#    stat_smooth(data=subset(georgia.df.max, Admin2=="Fulton"),aes(x=day,y=count,group=cutoff), method="lm")
```

For comparison, I'll pick an adjacent state that has not relaxed their restrictions.  Alabama seems a safe comparison as both South Carolina and Florida have begin partial openings (e.g., Florida opened beaches prior to April 24). One could also examine Tennessee and North Carolina, which also border with Georgia.

```{r warning=F, message=F, echo=longform}
# drop two counties because unassigned or out of GA; total 159 counties per wiki 
#https://en.wikipedia.org/wiki/List_of_counties_in_Georgia
alabama.df <- subset(datacov.US.long, `Province/State`=="Alabama") %>% filter(!Admin2 %in% c("Out of AL", "Unassigned") )

# all county plot
#ggplot(subset(alabama.df, day > as.Date("2020/04/1")), aes(day,count, group=Admin2, color=Admin2)) + geom_line() + ggtitle("Alabama counties") 

# focus just on counties with cases > 250 as of the compilation of the report
# red vertical line at 4/24 and blue vertical line at 4/27
alabama.df.max <- alabama.df %>% group_by(Admin2) %>% mutate(maxcount = max(count,na.rm=T)) %>% ungroup()

subset(alabama.df.max, day > as.Date("2020/04/1") & maxcount>=250) %>% 
  mutate(label = if_else(day.numeric == max(day.numeric), as.character(Admin2), NA_character_)) %>%
ggplot(aes(day,count, group=Admin2, color=Admin2)) + geom_line() + ggtitle("Alabama counties with greater than 250 cases")  + geom_vline(xintercept=as.Date("2020/04/24"), color="red") + ylim(range.ylim) + theme_bw() +
   geom_label_repel(aes(label= label), nudge_x=2, na.rm=T,segment.color = 'grey90', label.size=.01, size=2.5, show.legend=F) + scale_x_date(limits=c(as.Date("2020/04/1"), as.Date(max.day+5))) +
    theme(legend.position="none", plot.margin = margin(0.1, 1, 0.1, 0.1, "cm"))
```



## Hospital Readmission Data

My colleague Jack Iwashyna who you will learn more about in the concluding chapter, suggested a hypothesis in  mid April 2020.  [Research](https://www.ncbi.nlm.nih.gov/pubmed/28550403) suggests that 40% of  ARDS patients are re-admitted to the hospitial within 1 year of discharge. This estiamte came from a national, multicenter study with 839 ARDS survivors.  Most covid-19 deaths are related to ARDS (more on this in the concluding chapter).   Granted traditional ARDS patients may be different than covid-19-related ARDS patients but let's use this as a plausible estimate of hospital re-admission.  If we will see roughly a 40% chance of hospital readmission, it means that we may see additional hospitalizations in the coming months on top of the already steady stream of new covid-19 hospitalizations. And in the fall, if the predicted "second covid-19 wave" hits that could be compounded by additional hospital readmissions of people recently discharged or currently in the hospital. As the months progress, I'll present analyses examining this hypothesis along with models that will assess second waves.

The good news is that the study found that patient-reported physical activity and quality of life status is associated with fewer re-admissions.  So if we can get these  covid-19-related ARDS survivors walking and exercising we may have some hope. Unfortunately, many ARDS survivors have trouble merely standing up from a chair let alone going for a walk around the block wearing a mask.

More on this to come.


```{r eval=F, echo=F}
ggplot(subset(covid.tracking, date > as.Date("2020/03/20")), aes(date, hospitalizedCurrently, group=state, color=state)) + geom_line()

```

## Deaths

The SIR model can be explored in more detail and its implications can be examined. Here I'll focus on the simple SIR model without any of the  bells-and-whistles I discussed in Chapter \@ref(process).  The three primary equations of the SIR model are not easy to work with directly and they require numerical methods to approximate.

I'll pursue one approach that avoids numerical methods by making some assumptions that will  simplying these expressions.  As you will see, this simplification provides a reasonable fit to the data but their primary importance is in providing intuition into how to interpret these equations.

Following Keeling and Rohani (2008), 
if we assume that the $R_0$ is relatively small and that people interact randomly, then the incidence curve from the SIR model can be approximated with this form

$$
a \;\;\mbox{sech}^2 (\kappa_0 + \kappa_1 t)
$$
where sech is the hyperbolic secant, its argument is a linear transformation of time t with a slope and intercept denoted by $\kappa$s, and parameter $a$ is a multiplier.  These parameters are each functions of the parameters of the SIR model (the $\beta_1$ and $\beta_2$ from Chapter \@ref(process)) as well as the starting count at time 0. The hyperbolic secant can be reexpresssed in terms of exponentials:  $\mbox{sech} (x) = \frac{2}{\exp^{x} + \exp^{-x}}$.
To arrive at this expression one could work with the SIR equations presented in Chapter \@ref(process) directly,  but there is a simple way to get this form by assuming a Poisson distribution on the counts and showing that the probability that a randomly selected individual is not infected when the epidemic has an $R_0$ is $e^{\frac{-R_0}{N}}$ (i.e., a Poisson with k=0 and a rate of $R_0/N$); see Keeling and Rohani, 2008, Ch 2.


Let's perform a direct fit to the daily death count, since death is a more appriate outcome for the SIR model.  For this section I'll use the covid.tracking data (see Chapter \@ref(readintro)). We haven't worked much with that data yet so I'll also need to do some data cleaning and formating to be consistent with the other analyses conducted in this book.

```{r message=F, warning=F, results=ifelse(longform, "markup", "hide"), echo=longform}
# use this package just for sech 
library(pracma)

# define sir approximation to daily death count
richfu.sir <- function(d,a,b0,b1){
a*sech(b1*d + b0)^2
}

# use covid tracking data set; start at march 9 and create day.numeric as before
covid.tracking.new <- covid.tracking %>% subset(date>as.Date("2020-03-9", "%Y-%m-%d"))
# reorder the data.frame
covid.tracking.new <- arrange(covid.tracking.new, date, group_by=state)
covid.tracking.new$day.numeric <- as.numeric(covid.tracking.new$date)-18330

# temporary fix on 4/27/20 one missing code came up so I'll set it to 0
covid.tracking.new$deathIncrease <- ifelse(is.na(covid.tracking.new$deathIncrease),0,covid.tracking.new$deathIncrease)
```

```{r messages=F, warnings=F, results="hide", echo=longform}
# something wrong with day 57 so need to double check this; omit for now
covid.tracking.new <- subset(covid.tracking.new, day.numeric != 57)
out <- nlme(deathIncrease ~ richfu.sir(day.numeric, a, b0,b1), fixed = a+b0+b1~1,
            random =  a+b1~1, data=covid.tracking.new,start=c(a=47, b0=-2.5,b1=.1), verbose=T, groups=~state, control=nlmeControl(msMaxIter=500,opt="nlminb",pnlsTol=.001,msVerbose=T,minScale=.001, maxIter=500) )
```

```{r eval=F, echo=F}
#test function for additive constant
#doesn't work well, ranef correlates 1 with other parameters
richfu.sir2 <- function(d,a,b0,b1,b2){
b2 + a*sech(b1*d + b0)^2
}
out <- nlme(deathIncrease ~ richfu.sir2(day.numeric, a, b0,b1,b2), fixed = a+b0+b1+b2~1,
            random =  a+b1~1, data=covid.tracking.new,start=c(a=47.2, b0=-2.8,b1=.1, b2=500), verbose=T, groups=~state, control=nlmeControl(msMaxIter=500,opt="nlminb",pnlsTol=.001,msVerbose=T,minScale=.001, maxIter=500) )

```

We see that this function (red curve) does a reasonable job of fitting the daily death counts (black points) in New York. 
 There is noise in these data that this form of the SIR model cannot easily pickup.  
 
 
 
 
```{r  messages=F, warnings=F, echo=longform}
summary(out)
out.nlme.deaths <- out

# add predicted values to the data frame
covid.tracking.new$predicted <- predict(out, level=1)

ggplot( subset(covid.tracking.new, state=="NY"), aes(x=day.numeric,y=deathIncrease)) +  #geom_bar(stat="identity") +
  #  scale_x_date(date_breaks="1 week", date_labels = "%d %b") +
  geom_line(aes(day.numeric, predicted), color="red") + geom_point() +
    labs(y="Daily incidence ",
                                   title="Daily Covid-19 Deaths in  New York")  +
    theme(legend.position = "none",
          strip.text.y = element_text(size=11))
```

The data for death is quite noisy, for example, here are plots for both New Jersy and Michigan.

```{r  messages=F, warnings=F, echo=longform}
p1 <- ggplot( subset(covid.tracking.new, state=="NJ"), aes(x=day.numeric,y=deathIncrease)) +  #geom_bar(stat="identity") +
  #  scale_x_date(date_breaks="1 week", date_labels = "%d %b") +
  geom_line(aes(day.numeric, predicted), color="red") + geom_point() +
    labs(y="Daily incidence ",
                                   title="Daily Covid-19 Deaths in  New Jersey")  +
    theme(legend.position = "none",
          strip.text.y = element_text(size=11))

p2 <- ggplot( subset(covid.tracking.new, state=="MI"), aes(x=day.numeric,y=deathIncrease)) +  #geom_bar(stat="identity") +
  #  scale_x_date(date_breaks="1 week", date_labels = "%d %b") +
  geom_line(aes(day.numeric, predicted), color="red") + geom_point() +
    labs(y="Daily incidence ",
                                   title="Daily Covid-19 Deaths in  Michigan")  +
    theme(legend.position = "none",
          strip.text.y = element_text(size=11))

multiplot(p1,p2,cols=1)
```

But a more interesting result emerges by examining this approximate functional form.  We can integrate this approximation over t to see what form emerges  for the cumulative death count. In this context, integration is analogous to summing the daily counts to create a cumulative count but we do this symbolically on the function rather than the data in order to learn the form of the function on the cumulative count. 

The general form is 

$$
\bigg[\frac{a}{\kappa_1 (1+e^{2(\kappa_0 + \kappa_1 t)} )}\bigg] \;\;\; \bigg[(e^{2(\kappa_0 + \kappa_1 t)} - 1)\bigg] + C
$$
The left square bracket term has the form of the logistic growth function and the right square bracket term has the form of an exponential growth function. The term $C$ is the integration constant that we will set so that the cumulative sum starts at 0.  Thus, this simplification of the SIR model yields a product of the two functions we explored in Chapter \@ref(descmodel).  It isn't one or the other as we explored earlier, but this model suggests the curves approximately follow a combination of both forms. This is an example of the advantage of doing some mathematical modeling to be able to examine the implications of your model.  In the earlier chapter we considered exponential and logistic growth models without careful attention to why we chose those functions.  This type of mathematical modeling allows you to justify the functional forms you want to test based on implications from your hypotheses. Here I'm referring to the change equations of the SIR model as a set of hypotheses that govern how change occurs among the susceptibles, the infecteds and recovered.  Those equations (with some additional assumptions and approximations) imply this product of both forms, which we can test and exmaine with data.  Not counting the integration constant this approximation also has 3 parameters as we had in both the exponential form and the logistic growth form in Chapter \@ref(descmodel).

```{r warning=F, message=F, echo=longform}
# pull out the coefficients from the nlme 
coef.ny <- coef(out)[rownames(coef(out))=="NY",]
a <- unlist(coef.ny[1])
k0 <- unlist(coef.ny[2])
k1 <- unlist(coef.ny[3])
t <- (covid.tracking.new$day.numeric[covid.tracking.new$state=="NY"])
pred <- a*(exp(2*k1*t + 2*k0) - 1)/(k1*(exp(2*k1*t+2*k0)+1))
# set integration constant so incidence at time 6 is 3 (actual NY)
#pred <- pred + (-1*pred[length(pred)-5]) + 3 - 500
#different approach for integration constant; need to check, seems ad hoc
const <- coef(lm(covid.tracking.new$death[covid.tracking.new$state=="NY"]-pred ~ 1))
pred <- pred + const

##compare to computed cumsum directly
#deathny <- rev(covid.tracking.new$death[covid.tracking.new$state=="NY"])
#predcumsum <- cumsum(rev(predict(out)[names(predict(out))=="NY"]))
#cbind(deathny, pred,predcumsum)

preddat <- data.frame(day.numeric=t, day=covid.tracking.new$day.numeric[covid.tracking.new$state=="NY"], pred=pred)
ggplot(subset(covid.tracking.new, state=="NY"), aes(x=day.numeric,y=death)) +  
  geom_line(data=preddat, aes(day.numeric, pred), color="red") +
  geom_point() +
    labs(y="Cumulative counts ",
                                   title="US Covid-19 Deaths in New York")  +
    theme(legend.position = "none",
          strip.text.y = element_text(size=11))
```

The fit is ok given the simplying assumptions that were made to derive this form, such as assuming that $R_0$ is small, independence in transmission and some of the approximations that were used to derive this form (see Keeling & Rohani, 2008). We can reject a Poisson outright because the $R_0$ value likely changed over time with different levels of social distancing.

It is helpful to estimate the logistic function directly on the cumulative death count  and superimpose that curve (blue) on the same graph.  We see that the pure logistic provides a better fit to these data. This shouldn't be too surprising since I fit the logistic form directly to the data, whereas the red curve corresponding to the approximate SIR model takes into account other processes, has additional simplifying assumptions and approximations. Nonetheless, we can use the data and the fit to the models, to decide if we want to reject the SIR model because that model implies a functional form that fits worse than a simpler logistic growth model, use this finding to decide whether we should relax those simplying assumptions and derive a new form, or use this finding to justify using the SIR model because the fit to the data is "close enough." Of course, close enough depends on different applications. If one wants to describe a bunch of points with a simple curve or one wants to use the model to make forecasts.  In the case of forecasting to inform public policy, then "close enough" better be pretty darn close because many people's lives will be affected.  Often, the disagreements between modelers about their predictions boils down to details of the assumptions each modeler makes and the kinds of simplifications they use.

```{r warning=F, message=F, echo=longform, eval=F}
# logistic on death; use same function as used for logistic growth on positive count data
temp <- covid.tracking.new[,c("day.numeric","death","state")]
out.logistic.death <- nlme(death ~ richfu.logistic(day.numeric, l,y,b1), fixed = l+y+b1~1,
            random =  l+y~1, data=na.omit(temp),start=c( l=1000,y=100,b1=-.2), verbose=T, groups=~state, control=nlmeControl(msMaxIter=1000,opt="nlminb",pnlsTol=.01,msVerbose=T,minScale=.01, maxIter=1000, eval.mas=1000) )

# add predicted values from the logistic regression; need to fix scaling
preddat$pred.logistic <- c(rep(NA,5),rev(predict(out.logistic.death)[names(predict(out.logistic.death))=="NY"]))
ggplot(subset(covid.tracking.new, state=="NY"), aes(x=day.numeric,y=death)) +
 geom_line(data=preddat, aes(day.numeric, pred), color="red") +
  geom_point() + geom_line(data=preddat, aes(day.numeric, pred.logistic), color="blue")
    labs(y="Cumulative counts ",
                                   title="US Covid-19 Deaths New York")  +
    theme(legend.position = "none",
          strip.text.y = element_text(size=11))
```

```{r eval=F, ech0=F}
#testing weibull
covid.tracking.new <- groupedData(deathIncrease ~ day.numeric | state, data=covid.tracking.new)
initialvals <- getInitial(death ~ SSweibull(day.numeric, Asym, Drop, lrc, pwr), data=covid.tracking.new)

tempdata <- na.omit(data.frame(covid.tracking.new[,c("day.numeric", "death", "state")]))


out <- nlme(death ~ SSweibull(day.numeric, Asym, Drop, lrc, pwr), fixed = Asym +  Drop+ lrc+ pwr~1, random= Asym + pwr ~1, groups=~state,
            data=tempdata, start=initialvals, verbose=T, control=nlmeControl(msMaxIter=500,opt="nlminb",pnlsTol=.1,msVerbose=T,minScale=.1, maxIter=500) )

plot(1:60, SSweibull(1:60, Asym=initialvals[1],Drop=initialvals[2],lrc=initialvals[3],pwr=initialvals[4] ),ylim=c(0,2000))
points(unique(tempdata$day.numeric),c(unlist(by(tempdata$death,tempdata$day.numeric, mean,na.rm=T)))
       ,col="blue")

out <- nlme(deathIncrease ~ richfu.sir2(day.numeric, a, b0,b1,b2), fixed = a+b0+b1+b2~1,
            random =  a+b1~1, data=covid.tracking.new,start=c(a=47.2, b0=-2.8,b1=.1, b2=500), verbose=T, groups=~state, control=nlmeControl(msMaxIter=500,opt="nlminb",pnlsTol=.001,msVerbose=T,minScale=.001, maxIter=500) )
```
